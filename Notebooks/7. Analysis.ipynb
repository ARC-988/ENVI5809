{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd3e621-b281-4ce5-bdd4-01002dff15b3",
   "metadata": {},
   "source": [
    "## 0) Imports & constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d58c6320-d290-43be-80a3-23295b31c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset as NC4Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy import stats\n",
    "from matplotlib.dates import DateFormatter\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- Paths (edit to your layout)\n",
    "cyclone_path = \"raw_data/cyclone_data/IBTrACS.last3years.v04r01.nc\"\n",
    "hindcast_dir = \"raw_data/wave_hindcast_raw\"\n",
    "buoy_csv     = \"raw_data/OTI_wave_buoy/NSWENV_20250121-20250323_20m_OTInth2_WAVE_Parameters.csv\"\n",
    "cds_nc       = \"raw_data/CDS/wind_bbox.nc\"\n",
    "STORM        = \"ALFRED\"  # target storm name (case-insensitive)\n",
    "\n",
    "# ---- Time window (UTC)\n",
    "t0 = \"2025-02-01\"\n",
    "t1 = \"2025-03-31 23:59:59\"\n",
    "\n",
    "# ---- Physical constants\n",
    "R_EARTH_KM = 6371.0\n",
    "rho, g = 1025.0, 9.81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808c928-8632-4b35-9a66-04a7b503a326",
   "metadata": {},
   "source": [
    "## 1) Generic helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e8b5c29-6854-4cd5-9c68-fabdaf6593f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_var(ds, aliases):\n",
    "    \"\"\"Return first existing variable name from a list of aliases.\"\"\"\n",
    "    for name in aliases:\n",
    "        if name in ds:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Scalar haversine distance in km.\"\"\"\n",
    "    la1, lo1 = np.radians(lat1), np.radians(lon1)\n",
    "    la2, lo2 = np.radians(lat2), np.radians(lon2)\n",
    "    dlat, dlon = la2 - la1, lo2 - lo1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(la1)*np.cos(la2)*np.sin(dlon/2.0)**2\n",
    "    return 2 * R_EARTH_KM * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def initial_bearing_deg(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Scalar initial bearing (deg, 0..360).\"\"\"\n",
    "    la1, lo1 = np.radians(lat1), np.radians(lon1)\n",
    "    la2, lo2 = np.radians(lat2), np.radians(lon2)\n",
    "    dlon = lo2 - lo1\n",
    "    y = np.sin(dlon) * np.cos(la2)\n",
    "    x = np.cos(la1)*np.sin(la2) - np.sin(la1)*np.cos(la2)*np.cos(dlon)\n",
    "    return (np.degrees(np.arctan2(y, x)) + 360.0) % 360.0\n",
    "\n",
    "def _haversine_nan(lat0, lon0, lat_arr, lon_arr):\n",
    "    \"\"\"Vectorized haversine vs a fixed point, respects NaNs.\"\"\"\n",
    "    la = np.radians(lat_arr.astype(float))\n",
    "    lo = np.radians(lon_arr.astype(float))\n",
    "    la0, lo0 = np.radians(lat0), np.radians(lon0)\n",
    "    dlat, dlon = la - la0, lo - lo0\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(la0)*np.cos(la)*np.sin(dlon/2.0)**2\n",
    "    return 2 * R_EARTH_KM * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def _bearing_nan(lat0, lon0, lat_arr, lon_arr):\n",
    "    \"\"\"Vectorized initial bearing vs a fixed point, respects NaNs.\"\"\"\n",
    "    la = np.radians(lat_arr.astype(float))\n",
    "    lo = np.radians(lon_arr.astype(float))\n",
    "    la0, lo0 = np.radians(lat0), np.radians(lon0)\n",
    "    dlon = lo - lo0\n",
    "    y = np.sin(dlon) * np.cos(la)\n",
    "    x = np.cos(la0)*np.sin(la) - np.sin(la0)*np.cos(la)*np.cos(dlon)\n",
    "    return (np.degrees(np.arctan2(y, x)) + 360.0) % 360.0\n",
    "\n",
    "def norm_lon_180(lon):\n",
    "    \"\"\"Normalize longitudes to [-180, 180).\"\"\"\n",
    "    return ((np.asarray(lon, dtype='float64') + 180.0) % 360.0) - 180.0\n",
    "\n",
    "def circ_mean_deg(a):\n",
    "    \"\"\"Circular mean (deg) with NaN handling.\"\"\"\n",
    "    a = np.asarray(a, dtype='float64')\n",
    "    a = a[~np.isnan(a)]\n",
    "    if len(a) == 0: return np.nan\n",
    "    rad = np.deg2rad(a)\n",
    "    return (np.degrees(np.arctan2(np.nanmean(np.sin(rad)), np.nanmean(np.cos(rad)))) + 360.0) % 360.0\n",
    "\n",
    "def circ_diff(a, b):\n",
    "    \"\"\"Smallest signed angular difference b–a (deg, −180..180).\"\"\"\n",
    "    if np.isnan(a) or np.isnan(b): return np.nan\n",
    "    return ((b - a + 540) % 360) - 180\n",
    "\n",
    "def _bytes_to_str(a):\n",
    "    \"\"\"Decode bytes to str; leave str as-is.\"\"\"\n",
    "    if isinstance(a, (bytes, bytearray)):\n",
    "        return a.decode('utf-8', 'ignore')\n",
    "    return str(a)\n",
    "\n",
    "def _match_storm_mask(name_da, target):\n",
    "    \"\"\"Case-insensitive exact storm name match (handles bytes).\"\"\"\n",
    "    target_up = str(target).strip().upper()\n",
    "    v = np.array([_bytes_to_str(x).strip().upper() for x in name_da.values])\n",
    "    return v == target_up\n",
    "\n",
    "def _extract_time(st):\n",
    "    \"\"\"Robust time extraction from IBTrACS slice.\"\"\"\n",
    "    if 'time' in st.coords:\n",
    "        return pd.to_datetime(st['time'].values)\n",
    "    for cand in ['iso_time','ISO_TIME']:\n",
    "        if cand in st.variables:\n",
    "            arr = st[cand].astype(str).values\n",
    "            arr = np.where(pd.Series(arr).str.strip().astype(bool), arr, np.nan)\n",
    "            return pd.to_datetime(arr, errors='coerce')\n",
    "    for cand in ['date_time','DATE_TIME']:\n",
    "        if cand in st.variables:\n",
    "            try:\n",
    "                return pd.to_datetime(xr.decode_cf(st[[cand]])[cand].values)\n",
    "            except Exception:\n",
    "                dt = st[cand]\n",
    "                units = dt.attrs.get('units', 'days since 1858-11-17 00:00:00')\n",
    "                base  = pd.Timestamp(units.split('since',1)[1].strip()) if 'since' in units else pd.Timestamp('1858-11-17')\n",
    "                unit0 = units.split()[0].lower() if ' ' in units else 'days'\n",
    "                unit  = 'D' if unit0.startswith('day') else ('h' if unit0.startswith('hour') else 's')\n",
    "                return base + pd.to_timedelta(dt.values, unit=unit)\n",
    "    raise ValueError(\"No usable time in IBTrACS (looked for: time, iso_time, date_time).\")\n",
    "\n",
    "def _clean_indexed(df):\n",
    "    \"\"\"Ensure datetime index, uniqueness, sorted; combine duplicates by mean.\"\"\"\n",
    "    s = df.copy()\n",
    "    s.index = pd.to_datetime(s.index, errors='coerce')\n",
    "    s = s[~s.index.isna()].sort_index()\n",
    "    if not s.index.is_unique:\n",
    "        s = s.groupby(level=0).mean(numeric_only=True)\n",
    "    return s\n",
    "\n",
    "\n",
    "# --- Global figure saving setup ---\n",
    "SAVE_FIGS = True               # toggle ON/OFF\n",
    "FIG_DIR   = \"figures_out\"      # where to save\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "def save_all_open_figs(prefix=\"fig\", fmt=\"png\", close=True):\n",
    "    \"\"\"Save all currently open matplotlib figures to disk.\"\"\"\n",
    "    for i, num in enumerate(plt.get_fignums(), 1):\n",
    "        fig = plt.figure(num)\n",
    "        fname = f\"{prefix}_{i:02d}.{fmt}\"\n",
    "        path = os.path.join(FIG_DIR, fname)\n",
    "        fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Saved: {path}\")\n",
    "        if close:\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d8e187-41b5-4911-b66b-555b2a45568e",
   "metadata": {},
   "source": [
    "## 2) Load & harmonise hindcast (waves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002c2981-4f35-40bc-8b05-9348dbaba181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping broken file: raw_data/wave_hindcast_raw\\waves_point_444428.0_lon152.267_lat-23.733_202503.nc\n"
     ]
    }
   ],
   "source": [
    "# Discover valid .nc files under hindcast_dir\n",
    "raw_files = [os.path.join(root, f)\n",
    "             for root, _, files in os.walk(hindcast_dir)\n",
    "             for f in files if f.endswith(\".nc\")]\n",
    "\n",
    "valid_files = []\n",
    "for f in raw_files:  # Skip corrupted netCDFs\n",
    "    try:\n",
    "        with NC4Dataset(f) as _:\n",
    "            pass\n",
    "        valid_files.append(f)\n",
    "    except OSError:\n",
    "        print(f\"Skipping broken file: {f}\")\n",
    "\n",
    "if not valid_files:\n",
    "    raise FileNotFoundError(\"No valid hindcast .nc files found.\")\n",
    "\n",
    "# Open all and concat along a synthetic 'file' dim\n",
    "datasets = [xr.open_dataset(f, decode_times=True) for f in valid_files]\n",
    "ds_wave = xr.concat(datasets, dim=\"file\")\n",
    "\n",
    "# Normalize coord names to lat/lon\n",
    "rename_map = {}\n",
    "if 'longitude' in ds_wave.coords and 'lon' not in ds_wave.coords: rename_map['longitude'] = 'lon'\n",
    "if 'latitude'  in ds_wave.coords and 'lat' not in ds_wave.coords:  rename_map['latitude']  = 'lat'\n",
    "ds_wave = ds_wave.rename(rename_map)\n",
    "\n",
    "# Core vars via aliases\n",
    "hs_name  = pick_var(ds_wave, ['hs','Hm0','Hsig','Hm0_wave','significant_wave_height'])\n",
    "tp_name  = pick_var(ds_wave, ['tp','Tp','tpeak','t02','Tm02'])\n",
    "fp_name  = pick_var(ds_wave, ['fp','Fp','fpeak'])\n",
    "dir_name = pick_var(ds_wave, ['dir','Dp','dp','direction'])\n",
    "cg_name  = pick_var(ds_wave, ['cge','cg','C_g','group_velocity'])\n",
    "\n",
    "core_vars = [v for v in [hs_name, tp_name, fp_name, dir_name, cg_name] if v is not None]\n",
    "if not core_vars: raise ValueError(\"No wave variables found (hs/tp/fp/dir/cg).\")\n",
    "if 'time' not in ds_wave.coords: raise ValueError(\"Hindcast dataset has no 'time' coordinate.\")\n",
    "\n",
    "# Time slice and -> tidy DataFrame\n",
    "ds_wave = ds_wave.sel(time=slice(t0, t1))[core_vars]\n",
    "df_wave = (ds_wave.to_dataframe().reset_index().set_index('time').sort_index())\n",
    "\n",
    "# Harmonize variable names for downstream\n",
    "rename_cols = {}\n",
    "if hs_name:  rename_cols[hs_name]  = 'hs'\n",
    "if tp_name:  rename_cols[tp_name]  = 'tp' if 'tp' in tp_name.lower() else 't02'\n",
    "if fp_name:  rename_cols[fp_name]  = 'fp'\n",
    "if dir_name: rename_cols[dir_name] = 'dp'\n",
    "if cg_name:  rename_cols[cg_name]  = 'cge'\n",
    "df_wave = df_wave.rename(columns=rename_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e23d3-d1df-4e95-957a-d43ab115b140",
   "metadata": {},
   "source": [
    "## 3) Load & harmonise ERA5 (wind/pressure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7718b98c-856f-46da-a589-96aa006c5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cds = xr.open_dataset(cds_nc, decode_times=True)\n",
    "\n",
    "coord_renames = {}\n",
    "if 'valid_time' in ds_cds.coords and 'time' not in ds_cds.coords: coord_renames['valid_time'] = 'time'\n",
    "if 'latitude'   in ds_cds.coords and 'lat' not in ds_cds.coords:   coord_renames['latitude']   = 'lat'\n",
    "if 'longitude'  in ds_cds.coords and 'lon' not in ds_cds.coords:   coord_renames['longitude']  = 'lon'\n",
    "ds_cds = ds_cds.rename(coord_renames)\n",
    "\n",
    "u_name  = pick_var(ds_cds, ['u10','10u','u'])\n",
    "v_name  = pick_var(ds_cds, ['v10','10v','v'])\n",
    "sp_name = pick_var(ds_cds, ['sp','surface_pressure'])\n",
    "need = [u_name, v_name, sp_name]\n",
    "if any(v is None for v in need):\n",
    "    raise ValueError(f\"Missing ERA5 vars. Found u={u_name}, v={v_name}, sp={sp_name}\")\n",
    "if 'time' not in ds_cds.coords:\n",
    "    raise ValueError(\"ERA5 dataset has no 'time' coord.\")\n",
    "\n",
    "# Trim and clean admin dims/coords\n",
    "ds_cds = ds_cds.sel(time=slice(t0, t1))[need]\n",
    "for maybe_admin in ['expver','number','step','surface']:\n",
    "    if maybe_admin in ds_cds.coords and maybe_admin not in ds_cds.dims:\n",
    "        ds_cds = ds_cds.drop_vars(maybe_admin)\n",
    "    if maybe_admin in ds_cds.dims and ds_cds.dims[maybe_admin] == 1:\n",
    "        ds_cds = ds_cds.isel({maybe_admin: 0})\n",
    "\n",
    "df_cds = (ds_cds.to_dataframe().reset_index().set_index('time').sort_index()\n",
    "          .rename(columns={u_name:'u10', v_name:'v10', sp_name:'sp_pa'}))\n",
    "df_cds['sp_hpa']  = df_cds['sp_pa'] / 100.0\n",
    "df_cds['wind_ms'] = np.sqrt(df_cds['u10']**2 + df_cds['v10']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b5c41-9b5d-44ac-aba7-8ff9728c0d63",
   "metadata": {},
   "source": [
    "## 4) Load IBTrACS & isolate target storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399191c8-fd6f-4452-880a-7d1b3390e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBTrACS rows for ALFRED in window: 129\n",
      "Cyclone native window: 2025-02-21 00:00:00 → 2025-03-08 18:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arc\\miniconda3\\envs\\envireef_core\\lib\\site-packages\\xarray\\coding\\times.py:242: RuntimeWarning: invalid value encountered in cast\n",
      "  flat_num_dates_ns_int = (flat_num_dates * _NS_PER_TIME_DELTA[delta]).astype(\n",
      "C:\\Users\\arc\\miniconda3\\envs\\envireef_core\\lib\\site-packages\\xarray\\coding\\times.py:242: RuntimeWarning: invalid value encountered in cast\n",
      "  flat_num_dates_ns_int = (flat_num_dates * _NS_PER_TIME_DELTA[delta]).astype(\n"
     ]
    }
   ],
   "source": [
    "ds_cyc = xr.open_dataset(cyclone_path, decode_times=True)\n",
    "if 'name' not in ds_cyc: raise ValueError(\"IBTrACS lacks 'name' variable.\")\n",
    "\n",
    "mask = _match_storm_mask(ds_cyc['name'], STORM)\n",
    "if not mask.any(): raise ValueError(f\"Storm '{STORM}' not found in IBTrACS.\")\n",
    "st = ds_cyc.sel(storm=mask).squeeze(drop=True)\n",
    "\n",
    "lat_var = 'lat' if 'lat' in st.variables else ('latitude' if 'latitude' in st.variables else None)\n",
    "lon_var = 'lon' if 'lon' in st.variables else ('longitude' if 'longitude' in st.variables else None)\n",
    "if not lat_var or not lon_var:\n",
    "    raise ValueError(f\"IBTrACS lat/lon not found. Have: {list(st.variables)}\")\n",
    "\n",
    "lat_raw  = st[lat_var].values.astype('float64')\n",
    "lon_raw  = st[lon_var].values.astype('float64')\n",
    "time_raw = _extract_time(st)\n",
    "\n",
    "valid = np.isfinite(lat_raw) & np.isfinite(lon_raw) & pd.notna(time_raw)\n",
    "df_cyc = (pd.DataFrame({\n",
    "    'time':   pd.to_datetime(np.asarray(time_raw)[valid]),\n",
    "    'cyc_lat': lat_raw[valid],\n",
    "    'cyc_lon': norm_lon_180(lon_raw[valid]),\n",
    "}).sort_values('time')\n",
    "  .loc[lambda d: (d['time'] >= pd.Timestamp(t0)) & (d['time'] <= pd.Timestamp(t1))]\n",
    "  .reset_index(drop=True)\n",
    ")\n",
    "print(f\"IBTrACS rows for {STORM} in window:\", len(df_cyc))\n",
    "\n",
    "# Hourly cyclone window for masking outside true existence\n",
    "df_cyc_hr_native = (df_cyc.set_index('time').resample('1H').mean()\n",
    "                    .interpolate('time', limit_direction='both'))\n",
    "t_first = df_cyc_hr_native.index.min()\n",
    "t_last  = df_cyc_hr_native.index.max()\n",
    "print(\"Cyclone native window:\", t_first, \"→\", t_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6b104-574d-4520-8924-aca115f984f1",
   "metadata": {},
   "source": [
    "## 5) Sector geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244962ec-f9b3-4c56-9bc3-9493e23aa604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sector  sector_lat  sector_lon\n",
      "0    North  -22.722087      152.30\n",
      "1  Central  -23.233200      152.15\n",
      "2    South  -23.744314      152.05\n"
     ]
    }
   ],
   "source": [
    "SECTOR_BANDS = {\n",
    "    'North':   {'lat_min': -22.97764333, 'lat_max': -22.46652985},\n",
    "    'Central': {'lat_min': -23.48875682, 'lat_max': -22.97764333},\n",
    "    'South':   {'lat_min': -23.9998703,  'lat_max': -23.48875682},\n",
    "}\n",
    "# Swapped lon centers accordingly\n",
    "SECTOR_LON_CENTERS = {'North': 152.30, 'Central': 152.15, 'South': 152.05}\n",
    "\n",
    "def _mid_lat(band): return 0.5*(band['lat_min'] + band['lat_max'])\n",
    "\n",
    "sector_centroids = pd.DataFrame(\n",
    "    [{'sector': name, 'sector_lat': _mid_lat(band), 'sector_lon': SECTOR_LON_CENTERS[name]}\n",
    "     for name, band in SECTOR_BANDS.items()]\n",
    ")\n",
    "\n",
    "SECTOR_BOUNDS = {\n",
    "    k: {'lat_min': v['lat_min'], 'lat_max': v['lat_max'], 'lon_min': 151.70, 'lon_max': 152.50}\n",
    "    for k, v in SECTOR_BANDS.items()\n",
    "}\n",
    "print(sector_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df340a-02bd-48cf-93ae-c3f84f89d4a4",
   "metadata": {},
   "source": [
    "## 6) Build master hourly index & aligned series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e3e58ec-9992-4b96-b522-8eb33a4f7488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows:\n",
      " Wave:    2025-02-01 00:00:00 → 2025-04-01 00:00:00 | rows: 1417\n",
      " ERA5:    2025-02-01 00:00:00 → 2025-04-01 00:00:00 | rows: 1417\n",
      " Cyclone: 2025-02-21 00:00:00 → 2025-03-08 18:00:00 | rows: 379\n",
      " Master:  2025-02-01 00:00:00 → 2025-04-01 00:00:00 | rows: 1417\n"
     ]
    }
   ],
   "source": [
    "# Wave → average across grid: one series per timestamp\n",
    "wave_cols = [c for c in ['hs','tp','t02','dp','cge','fp'] if c in df_wave.columns]\n",
    "df_wave_ts = (df_wave[wave_cols].groupby(level='time').mean(numeric_only=True).sort_index())\n",
    "df_wave_ts['tp_use'] = df_wave_ts['tp'] if 'tp' in df_wave_ts.columns else df_wave_ts.get('t02', np.nan)\n",
    "\n",
    "# ERA5 → average across bbox to one series\n",
    "cds_cols = [c for c in ['u10','v10','sp_pa','sp_hpa','wind_ms'] if c in df_cds.columns]\n",
    "df_cds_ts = (df_cds[cds_cols].groupby(level='time').mean(numeric_only=True).sort_index())\n",
    "df_cds_ts['pressure_anom'] = 1013.25 - (df_cds_ts['sp_hpa'] if 'sp_hpa' in df_cds_ts.columns else df_cds_ts['sp_pa']/100.0)\n",
    "\n",
    "# Clean indices\n",
    "df_wave_ts = _clean_indexed(df_wave_ts)\n",
    "df_cds_ts  = _clean_indexed(df_cds_ts)\n",
    "df_cyc_ix  = df_cyc.copy()\n",
    "df_cyc_ix['time'] = pd.to_datetime(df_cyc_ix['time'], errors='coerce')\n",
    "df_cyc_ix = df_cyc_ix.dropna(subset=['time']).sort_values('time').set_index('time')\n",
    "\n",
    "# Master hourly index for [t0..t1]\n",
    "master_index = pd.date_range(start=pd.Timestamp(t0).floor('H'),\n",
    "                             end=pd.Timestamp(t1).ceil('H'), freq='1H')\n",
    "\n",
    "# Align to master\n",
    "wave_hourly = (df_wave_ts.resample('1H').mean().reindex(master_index).interpolate('time', limit=6))\n",
    "cds_hourly  = (df_cds_ts.resample('1H').mean().reindex(master_index).interpolate('time', limit=6))\n",
    "cyc_hourly_native = (df_cyc_ix[['cyc_lat','cyc_lon']].resample('1H').mean()\n",
    "                     .interpolate('time', limit_direction='both'))\n",
    "cyc_hourly = cyc_hourly_native.reindex(master_index)\n",
    "outside = (cyc_hourly.index < t_first) | (cyc_hourly.index > t_last)\n",
    "cyc_hourly.loc[outside, ['cyc_lat','cyc_lon']] = np.nan\n",
    "\n",
    "# Combined aligned (before sectors)\n",
    "aligned = (wave_hourly.join(cds_hourly, how='left', rsuffix='_cds')\n",
    "           .join(cyc_hourly, how='left'))\n",
    "\n",
    "# Ensure derived forcings exist\n",
    "if 'wind_ms' not in aligned and {'u10','v10'} <= set(aligned.columns):\n",
    "    aligned['wind_ms'] = np.sqrt(aligned['u10']**2 + aligned['v10']**2)\n",
    "if 'pressure_anom' not in aligned:\n",
    "    if 'sp_hpa' in aligned:\n",
    "        aligned['pressure_anom'] = 1013.25 - aligned['sp_hpa']\n",
    "    elif 'sp_pa' in aligned:\n",
    "        aligned['pressure_anom'] = 1013.25 - (aligned['sp_pa'] / 100.0)\n",
    "\n",
    "# Reusable masks\n",
    "obs_start, obs_end = pd.Timestamp(t0), pd.Timestamp(t1)\n",
    "in_obs  = (aligned.index >= obs_start) & (aligned.index <= obs_end)\n",
    "in_evt  = (aligned.index >= t_first)   & (aligned.index <= t_last)\n",
    "pre_evt  = (aligned.index >= obs_start) & (aligned.index <  t_first)\n",
    "post_evt = (aligned.index >  t_last)    & (aligned.index <= obs_end)\n",
    "\n",
    "print(\"Windows:\")\n",
    "print(\" Wave:   \", wave_hourly.index.min(), \"→\", wave_hourly.index.max(), \"| rows:\", len(wave_hourly))\n",
    "print(\" ERA5:   \", cds_hourly.index.min(),  \"→\", cds_hourly.index.max(),  \"| rows:\", len(cds_hourly))\n",
    "print(\" Cyclone:\", t_first, \"→\", t_last,                            \"| rows:\", len(cyc_hourly_native))\n",
    "print(\" Master: \", master_index.min(), \"→\", master_index.max(),     \"| rows:\", len(master_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a541b488-e5bf-495e-81c8-ef4e3c045d54",
   "metadata": {},
   "source": [
    "## 9) Sector time series builder and regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80899fb9-5252-419c-b29e-d67fb329280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sector wave sanity check (hs means):\n",
      "  North  : hs mean=2.087  dp mean=103.2\n",
      "  Central: hs mean=2.000  dp mean=99.0\n",
      "  South  : hs mean=1.730  dp mean=91.5\n"
     ]
    }
   ],
   "source": [
    "def _sector_wave_timeseries(ds_wave, bounds, t0, t1, master_index, rename_cols):\n",
    "    \"\"\"Subset hindcast to bounds; hourly ts with harmonized names + tp_use.\"\"\"\n",
    "    latb = (ds_wave['lat'] >= bounds['lat_min']) & (ds_wave['lat'] <= bounds['lat_max'])\n",
    "    lonb = (ds_wave['lon'] >= bounds['lon_min']) & (ds_wave['lon'] <= bounds['lon_max'])\n",
    "    mask = (latb & lonb)\n",
    "    if mask.sum().item() == 0:\n",
    "        raise ValueError(f\"No hindcast grid points inside bounds: {bounds}\")\n",
    "    ds_sec = ds_wave.sel(file=mask).sel(time=slice(t0, t1))\n",
    "    df = (ds_sec.to_dataframe().reset_index().set_index('time').sort_index()).rename(columns=rename_cols)\n",
    "    wave_cols = [c for c in ['hs','tp','t02','dp','cge','fp'] if c in df.columns]\n",
    "    if not wave_cols:\n",
    "        raise ValueError(\"Sector subset has no wave variables after renaming.\")\n",
    "    df_ts = (df[wave_cols].groupby(level='time').mean(numeric_only=True).sort_index())\n",
    "    df_ts['tp_use'] = df_ts['tp'] if 'tp' in df_ts.columns else df_ts.get('t02', np.nan)\n",
    "    return (df_ts.resample('1H').mean().reindex(master_index).interpolate('time', limit=6))\n",
    "\n",
    "# Build sectors with distance/bearing/energy flux\n",
    "sector_frames = {}\n",
    "for _, srow in sector_centroids.iterrows():\n",
    "    sector = srow['sector']\n",
    "    lat0, lon0 = float(srow['sector_lat']), float(srow['sector_lon'])\n",
    "    bounds = SECTOR_BOUNDS[sector]\n",
    "\n",
    "    wave_sec = _sector_wave_timeseries(ds_wave, bounds, t0, t1, master_index, rename_cols)\n",
    "    ts = (wave_sec\n",
    "          .join(cds_hourly[['wind_ms','sp_hpa']] if 'sp_hpa' in cds_hourly.columns else cds_hourly[['wind_ms','sp_pa']], how='left')\n",
    "          .join(cyc_hourly[['cyc_lat','cyc_lon']], how='left'))\n",
    "\n",
    "    if 'sp_hpa' not in ts and 'sp_pa' in ts:\n",
    "        ts['sp_hpa'] = ts['sp_pa'] / 100.0\n",
    "    ts['pressure_anom'] = 1013.25 - ts['sp_hpa']\n",
    "\n",
    "    ts['dist_km'] = _haversine_nan(lat0, lon0, ts['cyc_lat'].values, ts['cyc_lon'].values)\n",
    "    ts['bearing'] = _bearing_nan(lat0, lon0, ts['cyc_lat'].values, ts['cyc_lon'].values)\n",
    "\n",
    "    ts['cge_use'] = ts['cge'] if 'cge' in ts.columns else (g/(4.0*np.pi)) * ts['tp_use']\n",
    "    if 'hs' in ts.columns:\n",
    "        ts['energy_flux'] = 0.5 * rho * g * (ts['hs']**2) * ts['cge_use']\n",
    "\n",
    "    keep = ['hs','tp_use','dp','wind_ms','pressure_anom','dist_km','bearing','energy_flux']\n",
    "    sector_frames[sector] = ts[[c for c in keep if c in ts.columns]]\n",
    "\n",
    "print(\"\\nSector wave sanity check (hs means):\")\n",
    "for s, ts in sector_frames.items():\n",
    "    print(f\"  {s:7s}: hs mean={ts['hs'].mean():.3f}  dp mean={ts['dp'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a23aa53-6612-4e0e-bcbf-6b4c2bafe25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Directional Shift Analysis (per sector) ===\n",
      "North     mean°  pre: 104.8  peak: 114.0  post:  96.3  Δ(pre→peak):   9.1  Δ(peak→post): -17.7  Δ(pre→post):  -8.6\n",
      "Central   mean°  pre: 101.1  peak: 107.5  post:  92.3  Δ(pre→peak):   6.4  Δ(peak→post): -15.2  Δ(pre→post):  -8.9\n",
      "South     mean°  pre:  94.7  peak:  96.6  post:  86.2  Δ(pre→peak):   1.9  Δ(peak→post): -10.4  Δ(pre→post):  -8.5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Directional Shift Analysis (per sector) ===\")\n",
    "rows_dir = []\n",
    "for sector, ts in sector_frames.items():\n",
    "    pre  = ts.loc[pre_evt,  'dp'].dropna() if 'dp' in ts else pd.Series(dtype=float)\n",
    "    peak = ts.loc[in_evt,   'dp'].dropna() if 'dp' in ts else pd.Series(dtype=float)\n",
    "    post = ts.loc[post_evt, 'dp'].dropna() if 'dp' in ts else pd.Series(dtype=float)\n",
    "    pre_m, peak_m, post_m = circ_mean_deg(pre), circ_mean_deg(peak), circ_mean_deg(post)\n",
    "    d_pre_peak  = circ_diff(pre_m,  peak_m)\n",
    "    d_peak_post = circ_diff(peak_m, post_m)\n",
    "    d_pre_post  = circ_diff(pre_m,  post_m)\n",
    "    print(f\"{sector:8s}  mean°  pre:{pre_m:6.1f}  peak:{peak_m:6.1f}  post:{post_m:6.1f}  \"\n",
    "          f\"Δ(pre→peak):{d_pre_peak:6.1f}  Δ(peak→post):{d_peak_post:6.1f}  Δ(pre→post):{d_pre_post:6.1f}\")\n",
    "    rows_dir.append({'sector': sector, 'pre_mean': pre_m, 'peak_mean': peak_m, 'post_mean': post_m,\n",
    "                     'Δ_pre_peak': d_pre_peak, 'Δ_peak_post': d_peak_post, 'Δ_pre_post': d_pre_post})\n",
    "df_dirshift = pd.DataFrame(rows_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "686e4ed0-f7b0-4df6-955d-4ceffdc1fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[North] Hs~dist: R2=0.371, n=379\n",
      "[North] Hs~wind+dist+press: R2=0.661, n=379\n",
      "\n",
      "[Central] Hs~dist: R2=0.187, n=379\n",
      "[Central] Hs~wind+dist+press: R2=0.685, n=379\n",
      "\n",
      "[South] Hs~dist: R2=0.054, n=379\n",
      "[South] Hs~wind+dist+press: R2=0.756, n=379\n",
      "Hs ~ dist: R²=0.053 | n=379\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.8850      0.118     15.916      0.000       1.652       2.118\n",
      "dist_km       -0.0007      0.000     -4.577      0.000      -0.001      -0.000\n",
      "==============================================================================\n",
      "Hs ~ wind: R²=0.277 | n=379\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.3339      0.093      3.588      0.000       0.151       0.517\n",
      "wind_ms        0.1058      0.009     12.021      0.000       0.088       0.123\n",
      "==============================================================================\n",
      "Hs ~ press: R²=0.039 | n=379\n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "const             1.2825      0.046     27.924      0.000       1.192       1.373\n",
      "pressure_anom     0.0446      0.011      3.904      0.000       0.022       0.067\n",
      "=================================================================================\n",
      "Hs ~ wind+dist+press: R²=0.470 | n=379\n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "const            -0.1636      0.266     -0.615      0.539      -0.687       0.359\n",
      "wind_ms           0.1430      0.008     17.178      0.000       0.127       0.159\n",
      "dist_km          -0.0001      0.000     -0.376      0.707      -0.001       0.000\n",
      "pressure_anom     0.1010      0.021      4.848      0.000       0.060       0.142\n",
      "=================================================================================\n",
      "Hs ~ dist + dist²: R²=0.185 | n=379\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          4.8017      0.390     12.319      0.000       4.035       5.568\n",
      "dist_km       -0.0097      0.001     -8.342      0.000      -0.012      -0.007\n",
      "dist_km2    6.084e-06    7.8e-07      7.800      0.000    4.55e-06    7.62e-06\n",
      "==============================================================================\n",
      "\n",
      "North — Hs vs single forcings (cyclone window)\n",
      "         wind_ms | R²=0.518 | n=379 | coef=   0.224 | p=1.142e-61\n",
      "   pressure_anom | R²=0.003 | n=379 | coef=   0.018 | p=3.221e-01\n",
      "              dp | R²=0.086 | n=379 | coef=   0.013 | p=5.543e-09\n",
      "\n",
      "Central — Hs vs single forcings (cyclone window)\n",
      "         wind_ms | R²=0.554 | n=379 | coef=   0.236 | p=3.919e-68\n",
      "   pressure_anom | R²=0.001 | n=379 | coef=   0.012 | p=5.264e-01\n",
      "              dp | R²=0.034 | n=379 | coef=   0.010 | p=3.005e-04\n",
      "\n",
      "South — Hs vs single forcings (cyclone window)\n",
      "         wind_ms | R²=0.671 | n=379 | coef=   0.232 | p=4.235e-93\n",
      "   pressure_anom | R²=0.003 | n=379 | coef=  -0.019 | p=2.576e-01\n",
      "              dp | R²=0.057 | n=379 | coef=   0.010 | p=2.701e-06\n",
      "const            5.388241e-01\n",
      "wind_ms          3.330971e-49\n",
      "dist_km          7.070568e-01\n",
      "pressure_anom    1.829468e-06\n",
      "dtype: float64\n",
      "North: mean P=3076520.0, max P=36253064.0\n",
      "Central: mean P=2695598.2, max P=35160916.0\n",
      "South: mean P=1341172.8, max P=18895936.0\n"
     ]
    }
   ],
   "source": [
    "def _ols_report(name, y, X):\n",
    "    \"\"\"Fit OLS, print compact summary table.\"\"\"\n",
    "    Xc = sm.add_constant(X, has_constant='add')\n",
    "    m  = sm.OLS(y, Xc, missing='drop').fit()\n",
    "    print(f\"{name}: R²={m.rsquared:.3f} | n={int(m.nobs)}\")\n",
    "    print(m.summary().tables[1])\n",
    "    return m\n",
    "\n",
    "def regress_hs_distance(ts):\n",
    "    \"\"\"Hs ~ dist_km (+ optional quadratic) during event.\"\"\"\n",
    "    d = ts.loc[in_evt, ['hs','dist_km']].dropna()\n",
    "    X = pd.DataFrame({'dist_km': d['dist_km'], 'dist_km2': d['dist_km']**2})\n",
    "    return sm.OLS(d['hs'], sm.add_constant(X)).fit()\n",
    "\n",
    "def regress_exposure(ts):\n",
    "    \"\"\"Hs ~ wind + dist + pressure during event.\"\"\"\n",
    "    cols = ['hs','wind_ms','pressure_anom','dist_km']\n",
    "    d = ts.loc[in_evt, cols].dropna()\n",
    "    return sm.OLS(d['hs'], sm.add_constant(d[['wind_ms','pressure_anom','dist_km']])).fit()\n",
    "\n",
    "# Sector regressions\n",
    "for sector, ts in sector_frames.items():\n",
    "    m1 = regress_hs_distance(ts)\n",
    "    m2 = regress_exposure(ts)\n",
    "    print(f\"\\n[{sector}] Hs~dist: R2={m1.rsquared:.3f}, n={int(m1.nobs)}\")\n",
    "    print(f\"[{sector}] Hs~wind+dist+press: R2={m2.rsquared:.3f}, n={int(m2.nobs)}\")\n",
    "\n",
    "# Buoy regressions\n",
    "df_reg = oti.loc[peak_mask, ['hs','dist_km','wind_ms','pressure_anom']].dropna()\n",
    "m_dist     = _ols_report(\"Hs ~ dist\",               df_reg['hs'], df_reg[['dist_km']])\n",
    "m_wind     = _ols_report(\"Hs ~ wind\",               df_reg['hs'], df_reg[['wind_ms']])\n",
    "m_press    = _ols_report(\"Hs ~ press\",              df_reg['hs'], df_reg[['pressure_anom']])\n",
    "m_exposure = _ols_report(\"Hs ~ wind+dist+press\",    df_reg['hs'], df_reg[['wind_ms','dist_km','pressure_anom']])\n",
    "df_reg_poly = df_reg.assign(dist_km2=lambda d: d['dist_km']**2)\n",
    "m_dist2     = _ols_report(\"Hs ~ dist + dist²\",      df_reg_poly['hs'], df_reg_poly[['dist_km','dist_km2']])\n",
    "\n",
    "# Single-var quick scan per sector\n",
    "def single_var_reg(ts, xvar):\n",
    "    cols = ['hs', xvar]\n",
    "    d = ts.loc[in_evt, cols].dropna()\n",
    "    if len(d) < 10: return None\n",
    "    m = sm.OLS(d['hs'], sm.add_constant(d[xvar])).fit()\n",
    "    return xvar, m.rsquared, len(d), m.params.get(xvar, np.nan), m.pvalues.get(xvar, np.nan)\n",
    "\n",
    "for sector, ts in sector_frames.items():\n",
    "    results = [single_var_reg(ts, var) for var in ['wind_ms', 'pressure_anom', 'dp']]\n",
    "    results = [r for r in results if r]\n",
    "    print(f\"\\n{sector} — Hs vs single forcings (cyclone window)\")\n",
    "    for var, r2, n, coef, p in results:\n",
    "        print(f\"  {var:>14} | R²={r2:5.3f} | n={n:3d} | coef={coef:8.3f} | p={p:8.3e}\")\n",
    "\n",
    "# Example access (kept as in your code)\n",
    "model = m_exposure\n",
    "print(model.pvalues)\n",
    "\n",
    "# Energy flux stats per sector (±3 d)\n",
    "win = (aligned.index >= (t_first - pad)) & (aligned.index <= (t_last + pad))\n",
    "for sector, ts in sector_frames.items():\n",
    "    sub = ts.loc[win]\n",
    "    if 'energy_flux' in sub:\n",
    "        print(f\"{sector}: mean P={sub['energy_flux'].mean():.1f}, max P={sub['energy_flux'].max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40aa147-8516-4501-ad64-11a9673d00c7",
   "metadata": {},
   "source": [
    "## 8) Load OTI buoy, build hourly series and regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5506038f-8a2c-4eda-a568-ef8cc73470b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_buoy shape: (2413, 18)\n"
     ]
    }
   ],
   "source": [
    "def _maybe_parse_datetime(col):\n",
    "    try:\n",
    "        return pd.to_datetime(col, errors='raise')\n",
    "    except Exception:\n",
    "        return pd.to_datetime(col, errors='coerce')\n",
    "\n",
    "df_buoy_raw = pd.read_csv(buoy_csv)\n",
    "time_candidates = [c for c in df_buoy_raw.columns if 'time' in c.lower() or 'date' in c.lower()]\n",
    "if not time_candidates:\n",
    "    raise ValueError(\"Buoy CSV missing a datetime column.\")\n",
    "time_col_buoy = time_candidates[0]\n",
    "\n",
    "df_buoy = df_buoy_raw.copy()\n",
    "df_buoy[time_col_buoy] = _maybe_parse_datetime(df_buoy[time_col_buoy])\n",
    "df_buoy = (df_buoy.loc[(df_buoy[time_col_buoy] >= pd.Timestamp(t0)) & (df_buoy[time_col_buoy] <= pd.Timestamp(t1))]\n",
    "                  .set_index(time_col_buoy).sort_index())\n",
    "\n",
    "# Rename likely columns → minimal set\n",
    "rename_buoy = {'Hm0':'hs','Hsig':'hs','Tp':'tp','Tz':'tz','Tm02':'t02','Dp':'dp','Dm':'dm','Dp_sp':'dp_sp','Dm_sp':'dm_sp','temp':'temp'}\n",
    "df_buoy = df_buoy.rename(columns={k:v for k,v in rename_buoy.items() if k in df_buoy.columns})\n",
    "df_buoy = df_buoy.loc[:, ~df_buoy.columns.duplicated()].copy()\n",
    "\n",
    "# Fill aliases if needed\n",
    "alias = {'Hs':'hs','Hm0':'hs','Hsig':'hs','Tp':'tp','Tz':'tz','Tm02':'t02','Dp':'dp','Dm':'dm','Dp_sp':'dp_sp','Dm_sp':'dm_sp','Temp':'temp'}\n",
    "for k,v in alias.items():\n",
    "    if k in df_buoy.columns and v not in df_buoy.columns:\n",
    "        df_buoy.rename(columns={k:v}, inplace=True)\n",
    "\n",
    "# OTI hourly series\n",
    "keep_buoy = [c for c in ['hs','tp','t02','dp','tz','temp'] if c in df_buoy.columns]\n",
    "buoy_hourly = (df_buoy[keep_buoy].resample('1H').mean(numeric_only=True)\n",
    "               .reindex(pd.date_range(pd.Timestamp(t0).floor('H'), pd.Timestamp(t1).ceil('H'), freq='1H'))\n",
    "               .interpolate('time', limit=4))\n",
    "buoy_hourly['tp_use'] = buoy_hourly['tp'] if 'tp' in buoy_hourly.columns else buoy_hourly.get('t02', np.nan)\n",
    "\n",
    "# Join forcings + cyclone; compute distance/bearing from OTI\n",
    "OTI = {'name': 'One Tree Island', 'lat': -23.504, 'lon': 152.094}\n",
    "oti = (buoy_hourly\n",
    "       .join(cds_hourly[['wind_ms','sp_hpa']] if 'sp_hpa' in cds_hourly.columns else cds_hourly[['wind_ms','sp_pa']], how='left')\n",
    "       .join(cyc_hourly[['cyc_lat','cyc_lon']], how='left'))\n",
    "if 'sp_hpa' not in oti and 'sp_pa' in oti:\n",
    "    oti['sp_hpa'] = oti['sp_pa']/100.0\n",
    "oti['pressure_anom'] = 1013.25 - oti['sp_hpa']\n",
    "oti['dist_km'] = _haversine_nan(OTI['lat'], OTI['lon'], oti['cyc_lat'].values, oti['cyc_lon'].values)\n",
    "oti['bearing'] = _bearing_nan(OTI['lat'], OTI['lon'], oti['cyc_lat'].values, oti['cyc_lon'].values)\n",
    "oti['cge_est'] = (g/(4.0*np.pi)) * oti['tp_use']\n",
    "oti['P_buoy']  = 0.5 * rho * g * (oti['hs']**2) * oti['cge_est']\n",
    "\n",
    "# Phase masks for buoy\n",
    "pad = pd.Timedelta(days=3)\n",
    "pre_mask  = (oti.index >= pd.Timestamp(t0)) & (oti.index <  t_first)\n",
    "peak_mask = (oti.index >= t_first) & (oti.index <= t_last)\n",
    "post_mask = (oti.index >  t_last) & (oti.index <= pd.Timestamp(t1))\n",
    "win_mask  = (oti.index >= (t_first - pad)) & (oti.index <= (t_last + pad))\n",
    "\n",
    "print(\"df_buoy shape:\", df_buoy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b1dbea-a20e-4dfd-98a6-d56f4fd20e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTI buoy sanity check:\n",
      "  OTI    : hs mean=1.120  dp mean=55.2\n"
     ]
    }
   ],
   "source": [
    "def _oti_buoy_timeseries(buoy_hourly, master_index, cds_hourly, cyc_hourly, OTI_lat, OTI_lon):\n",
    "    \"\"\"\n",
    "    OTI buoy -> hourly ts aligned to master_index with the same columns as sector frames:\n",
    "    ['hs','tp_use','dp','wind_ms','pressure_anom','dist_km','bearing','energy_flux']\n",
    "    \"\"\"\n",
    "    # 1) Start from buoy_hourly; ensure index is hourly and aligned\n",
    "    bu = (buoy_hourly\n",
    "          .reindex(master_index)                # align to master clock\n",
    "          .interpolate('time', limit=4))        # small gaps fill\n",
    "\n",
    "    # 2) tp_use (prefer Tp else Tm02)\n",
    "    if 'tp_use' not in bu.columns:\n",
    "        bu['tp_use'] = bu['tp'] if 'tp' in bu.columns else bu.get('t02', np.nan)\n",
    "\n",
    "    # 3) Join ERA5 wind/pressure + cyclone center\n",
    "    ts = (bu\n",
    "          .join(cds_hourly[['wind_ms','sp_hpa']] if 'sp_hpa' in cds_hourly.columns\n",
    "                else cds_hourly[['wind_ms','sp_pa']], how='left')\n",
    "          .join(cyc_hourly[['cyc_lat','cyc_lon']], how='left'))\n",
    "\n",
    "    # 4) Pressure anomaly (hPa)\n",
    "    if 'sp_hpa' not in ts and 'sp_pa' in ts:\n",
    "        ts['sp_hpa'] = ts['sp_pa'] / 100.0\n",
    "    ts['pressure_anom'] = 1013.25 - ts['sp_hpa']\n",
    "\n",
    "    # 5) Distance & bearing from OTI to cyclone\n",
    "    ts['dist_km'] = _haversine_nan(OTI_lat, OTI_lon, ts['cyc_lat'].values, ts['cyc_lon'].values)\n",
    "    ts['bearing'] = _bearing_nan(OTI_lat, OTI_lon, ts['cyc_lat'].values, ts['cyc_lon'].values)\n",
    "\n",
    "    # 6) Energy flux (deep-water Cg ≈ g/(4π)·T)\n",
    "    ts['cge_use'] = (g/(4.0*np.pi)) * ts['tp_use']\n",
    "    if 'hs' in ts.columns:\n",
    "        ts['energy_flux'] = 0.5 * rho * g * (ts['hs']**2) * ts['cge_use']\n",
    "\n",
    "    # 7) Keep same column set as sectors\n",
    "    keep = ['hs','tp_use','dp','wind_ms','pressure_anom','dist_km','bearing','energy_flux']\n",
    "    return ts[[c for c in keep if c in ts.columns]]\n",
    "\n",
    "# Build OTI frame\n",
    "OTI = {'name': 'One Tree Island', 'lat': -23.504, 'lon': 152.094}  # if not already defined\n",
    "oti_frame = _oti_buoy_timeseries(buoy_hourly, master_index, cds_hourly, cyc_hourly,\n",
    "                                 OTI_lat=OTI['lat'], OTI_lon=OTI['lon'])\n",
    "\n",
    "# (optional) register as another \"sector\" so downstream loops include OTI\n",
    "sector_frames['OTI'] = oti_frame\n",
    "\n",
    "# Quick sanity check (same as sectors)\n",
    "print(\"\\nOTI buoy sanity check:\")\n",
    "print(f\"  OTI    : hs mean={oti_frame['hs'].mean():.3f}  dp mean={oti_frame['dp'].mean():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f0c541-89e4-4b89-9030-0236b6252f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Directional Shift Analysis (OTI buoy) ===\n",
      "OTI      mean°  pre:  52.3  peak:  48.4  post:  51.6  Δ(pre→peak):  -3.9  Δ(peak→post):   3.1  Δ(pre→post):  -0.8\n",
      "         n      pre:   480  peak:   379  post:   352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series</th>\n",
       "      <th>pre_mean</th>\n",
       "      <th>peak_mean</th>\n",
       "      <th>post_mean</th>\n",
       "      <th>Δ_pre_peak</th>\n",
       "      <th>Δ_peak_post</th>\n",
       "      <th>Δ_pre_post</th>\n",
       "      <th>n_pre</th>\n",
       "      <th>n_peak</th>\n",
       "      <th>n_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OTI buoy</td>\n",
       "      <td>52.3</td>\n",
       "      <td>48.4</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>480</td>\n",
       "      <td>379</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     series  pre_mean  peak_mean  post_mean  Δ_pre_peak  Δ_peak_post  \\\n",
       "0  OTI buoy      52.3       48.4       51.6        -3.9          3.1   \n",
       "\n",
       "   Δ_pre_post  n_pre  n_peak  n_post  \n",
       "0        -0.8    480     379     352  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Directional Shift Analysis — OTI buoy ====================================\n",
    "print(\"\\n=== Directional Shift Analysis (OTI buoy) ===\")\n",
    "\n",
    "if 'dp' not in oti.columns:\n",
    "    print(\"[OTI] No 'dp' column found in buoy data — skipping.\")\n",
    "    df_dirshift_oti = pd.DataFrame(columns=['series','pre_mean','peak_mean','post_mean',\n",
    "                                            'Δ_pre_peak','Δ_peak_post','Δ_pre_post','n_pre','n_peak','n_post'])\n",
    "else:\n",
    "    pre_dp  = oti.loc[pre_mask,  'dp'].dropna()\n",
    "    peak_dp = oti.loc[peak_mask, 'dp'].dropna()\n",
    "    post_dp = oti.loc[post_mask, 'dp'].dropna()\n",
    "\n",
    "    pre_m  = circ_mean_deg(pre_dp.values)\n",
    "    peak_m = circ_mean_deg(peak_dp.values)\n",
    "    post_m = circ_mean_deg(post_dp.values)\n",
    "\n",
    "    d_pre_peak  = circ_diff(pre_m,  peak_m)\n",
    "    d_peak_post = circ_diff(peak_m, post_m)\n",
    "    d_pre_post  = circ_diff(pre_m,  post_m)\n",
    "\n",
    "    print(f\"OTI      mean°  pre:{pre_m:6.1f}  peak:{peak_m:6.1f}  post:{post_m:6.1f}  \"\n",
    "          f\"Δ(pre→peak):{d_pre_peak:6.1f}  Δ(peak→post):{d_peak_post:6.1f}  Δ(pre→post):{d_pre_post:6.1f}\")\n",
    "    print(f\"         n      pre:{len(pre_dp):6d}  peak:{len(peak_dp):6d}  post:{len(post_dp):6d}\")\n",
    "\n",
    "    # tidy summary frame (mirrors your sector df)\n",
    "    df_dirshift_oti = pd.DataFrame([{\n",
    "        'series': 'OTI buoy',\n",
    "        'pre_mean': pre_m, 'peak_mean': peak_m, 'post_mean': post_m,\n",
    "        'Δ_pre_peak': d_pre_peak, 'Δ_peak_post': d_peak_post, 'Δ_pre_post': d_pre_post,\n",
    "        'n_pre': len(pre_dp), 'n_peak': len(peak_dp), 'n_post': len(post_dp)\n",
    "    }])\n",
    "\n",
    "# optional: display rounded table\n",
    "try:\n",
    "    display(df_dirshift_oti.round(1))\n",
    "except Exception:\n",
    "    print(\"\\nOTI directional shift summary:\")\n",
    "    print(df_dirshift_oti.round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "29f7fb06-215a-47e5-8e3c-3bf9b5c4cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ols_report(name, y, X):\n",
    "    \"\"\"Fit OLS, print compact summary table (with full p-value precision).\"\"\"\n",
    "    Xc = sm.add_constant(X, has_constant='add')\n",
    "    m  = sm.OLS(y, Xc, missing='drop').fit()\n",
    "    print(f\"{name}: R²={m.rsquared:.3f} | n={int(m.nobs)}\")\n",
    "\n",
    "    # Rebuild our own table so p-values keep full precision\n",
    "    print(\"==============================================================================\")\n",
    "    print(f\"{'':20s} {'coef':>12s} {'std err':>12s} {'t':>10s} {'P>|t|':>14s} {'[0.025':>12s} {'0.975]':>12s}\")\n",
    "    print(\"------------------------------------------------------------------------------\")\n",
    "    for param in m.params.index:\n",
    "        coef = m.params[param]\n",
    "        se   = m.bse[param]\n",
    "        tval = m.tvalues[param]\n",
    "        pval = m.pvalues[param]\n",
    "        ci_low, ci_high = m.conf_int().loc[param]\n",
    "        print(f\"{param:20s} {coef:12.4f} {se:12.4f} {tval:10.4f} {pval:14.10f} {ci_low:12.4f} {ci_high:12.4f}\")\n",
    "    print(\"==============================================================================\")\n",
    "    return m\n",
    "\n",
    "\n",
    "def regress_hs_distance(ts, mask):\n",
    "    \"\"\"Hs ~ dist_km (+ dist_km²) within a mask (e.g., peak_mask).\"\"\"\n",
    "    d = ts.loc[mask, ['hs','dist_km']].dropna()\n",
    "    if len(d) < 5:\n",
    "        print(\"[Hs~dist] Not enough rows:\", len(d))\n",
    "        return None\n",
    "    X = pd.DataFrame({'dist_km': d['dist_km'], 'dist_km2': d['dist_km']**2})\n",
    "    return sm.OLS(d['hs'], sm.add_constant(X)).fit()\n",
    "\n",
    "\n",
    "def regress_exposure(ts, mask):\n",
    "    \"\"\"Hs ~ wind_ms + dist_km + pressure_anom within a mask.\"\"\"\n",
    "    cols = ['hs','wind_ms','pressure_anom','dist_km']\n",
    "    d = ts.loc[mask, cols].dropna()\n",
    "    if len(d) < 5:\n",
    "        print(\"[Hs~wind+dist+press] Not enough rows:\", len(d))\n",
    "        return None\n",
    "    return sm.OLS(d['hs'], sm.add_constant(d[['wind_ms','pressure_anom','dist_km']])).fit()\n",
    "\n",
    "\n",
    "def single_var_reg(ts, xvar, mask):\n",
    "    \"\"\"Quick single-var scan Hs ~ xvar within a mask.\"\"\"\n",
    "    cols = ['hs', xvar]\n",
    "    d = ts.loc[mask, cols].dropna()\n",
    "    if len(d) < 10:\n",
    "        return None\n",
    "    m = sm.OLS(d['hs'], sm.add_constant(d[xvar])).fit()\n",
    "    return xvar, m.rsquared, len(d), m.params.get(xvar, np.nan), m.pvalues.get(xvar, np.nan)\n",
    "\n",
    "\n",
    "def oti_regression_suite(oti_like, label=\"OTI buoy\", mask=None):\n",
    "    \"\"\"Run full regression suite on OTI (Hs ~ various forcings).\"\"\"\n",
    "    if mask is None:\n",
    "        mask = peak_mask\n",
    "\n",
    "    print(f\"\\n=== {label}: regressions ({'peak_mask' if mask is peak_mask else 'custom mask'}) ===\")\n",
    "\n",
    "    # Multi-var models\n",
    "    m1 = regress_hs_distance(oti_like, mask)\n",
    "    m2 = regress_exposure(oti_like, mask)\n",
    "    if m1 is not None:\n",
    "        print(f\"[{label}] Hs~dist: R2={m1.rsquared:.3f}, n={int(m1.nobs)}\")\n",
    "    if m2 is not None:\n",
    "        print(f\"[{label}] Hs~wind+dist+press: R2={m2.rsquared:.3f}, n={int(m2.nobs)}\")\n",
    "\n",
    "    # Explicit OLS tables (full precision p-values)\n",
    "    d = oti_like.loc[mask, ['hs','dist_km','wind_ms','pressure_anom']].dropna()\n",
    "    if len(d) >= 5:\n",
    "        _ = _ols_report(f\"{label} | Hs ~ dist\",               d['hs'], d[['dist_km']])\n",
    "        _ = _ols_report(f\"{label} | Hs ~ wind\",               d['hs'], d[['wind_ms']])\n",
    "        _ = _ols_report(f\"{label} | Hs ~ press\",              d['hs'], d[['pressure_anom']])\n",
    "        m_expo_oti = _ols_report(f\"{label} | Hs ~ wind+dist+press\", d['hs'], d[['wind_ms','dist_km','pressure_anom']])\n",
    "        d_poly = d.assign(dist_km2=lambda x: x['dist_km']**2)\n",
    "        _ = _ols_report(f\"{label} | Hs ~ dist + dist²\", d_poly['hs'], d_poly[['dist_km','dist_km2']])\n",
    "    else:\n",
    "        m_expo_oti = None\n",
    "        print(f\"[{label}] Not enough rows for printed OLS tables (n={len(d)}).\")\n",
    "\n",
    "    # Single-var scan\n",
    "    results = [single_var_reg(oti_like, var, mask) for var in ['wind_ms','pressure_anom','dp']]\n",
    "    results = [r for r in results if r]\n",
    "    if results:\n",
    "        print(f\"\\n{label} — Hs vs single forcings\")\n",
    "        for var, r2, n, coef, p in results:\n",
    "            print(f\"  {var:>14} | R²={r2:5.3f} | n={n:3d} | coef={coef:8.3f} | p={p:.10f}\")\n",
    "\n",
    "    # Energy flux summary (±3d)\n",
    "    win = (oti_like.index >= (t_first - pad)) & (oti_like.index <= (t_last + pad))\n",
    "    if 'energy_flux' in oti_like.columns:\n",
    "        sub = oti_like.loc[win, 'energy_flux'].dropna()\n",
    "        if len(sub):\n",
    "            print(f\"\\n{label}: mean P={sub.mean():.1f}, max P={sub.max():.1f} (±3d)\")\n",
    "        else:\n",
    "            print(f\"\\n{label}: no non-NaN energy_flux values in ±3d window.\")\n",
    "    else:\n",
    "        print(f\"\\n{label}: 'energy_flux' column not present.\")\n",
    "\n",
    "    return {'m_dist': m1, 'm_exposure': m2, 'm_exposure_table': m_expo_oti}\n",
    "\n",
    "\n",
    "# ---- Example execution (if running standalone) ----\n",
    "_ = oti_regression_suite(oti if 'oti' in locals() else oti_frame, label=\"OTI buoy\", mask=peak_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeafec8-1e63-4625-aefa-47ba8dc08541",
   "metadata": {},
   "source": [
    "## 9) Cross-correlation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40d9cc09-a527-4c48-89eb-c94bcfb1fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arc\\AppData\\Local\\Temp\\ipykernel_38204\\1609172813.py:20: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n",
      "  plt.stem(lags, cors, use_line_collection=True)\n",
      "C:\\Users\\arc\\AppData\\Local\\Temp\\ipykernel_38204\\1609172813.py:20: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n",
      "  plt.stem(lags, cors, use_line_collection=True)\n",
      "C:\\Users\\arc\\AppData\\Local\\Temp\\ipykernel_38204\\1609172813.py:20: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n",
      "  plt.stem(lags, cors, use_line_collection=True)\n",
      "C:\\Users\\arc\\AppData\\Local\\Temp\\ipykernel_38204\\1609172813.py:20: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n",
      "  plt.stem(lags, cors, use_line_collection=True)\n"
     ]
    }
   ],
   "source": [
    "def best_cross_correlation(x, y, max_lag_hours=48, title=''):\n",
    "    \"\"\"Lag with max |corr|; positive = x leads y.\"\"\"\n",
    "    df = pd.concat([x.rename('x'), y.rename('y')], axis=1).dropna()\n",
    "    if len(df) < 24:\n",
    "        print(f\"[{title}] Not enough data for cross-correlation (n={len(df)}).\")\n",
    "        return None\n",
    "    xz = (df['x'] - df['x'].mean()) / df['x'].std(ddof=1)\n",
    "    yz = (df['y'] - df['y'].mean()) / df['y'].std(ddof=1)\n",
    "    lags = np.arange(-max_lag_hours, max_lag_hours+1, 1)\n",
    "    cors = []\n",
    "    for L in lags:\n",
    "        if L < 0:  cors.append(np.corrcoef(xz[-L:].values, yz[:len(yz)+L].values)[0,1])\n",
    "        elif L>0:  cors.append(np.corrcoef(xz[:len(xz)-L].values, yz[L:].values)[0,1])\n",
    "        else:      cors.append(np.corrcoef(xz.values, yz.values)[0,1])\n",
    "    best_idx = int(np.nanargmax(np.abs(cors)))\n",
    "    best_lag = int(lags[best_idx]); best_corr = float(cors[best_idx])\n",
    "    sign = \"x leads y\" if best_lag > 0 else (\"y leads x\" if best_lag < 0 else \"simultaneous\")\n",
    "    print(f\"\\n{title}  best lag = {best_lag:+d} h ({sign}),  corr = {best_corr:.3f}\")\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.stem(lags, cors, use_line_collection=True)\n",
    "    plt.xlabel('Lag (hours): positive = wind leads Hs'); plt.ylabel('Correlation'); plt.title(f'Cross-correlation: {title}')\n",
    "    plt.tight_layout()\n",
    "    return best_lag, best_corr\n",
    "\n",
    "def crosscorr_plot(x, y, max_lag_hours=48, title='Cross-correlation'):\n",
    "    \"\"\"Plot only, same lag convention as above.\"\"\"\n",
    "    df = pd.concat([x.rename('x'), y.rename('y')], axis=1).dropna()\n",
    "    if len(df) < 24:\n",
    "        print(f\"[{title}] Not enough data.\")\n",
    "        return\n",
    "    xz = (df['x'] - df['x'].mean()) / df['x'].std(ddof=1)\n",
    "    yz = (df['y'] - df['y'].mean()) / df['y'].std(ddof=1)\n",
    "    lags = np.arange(-max_lag_hours, max_lag_hours+1, 1)\n",
    "    cors = []\n",
    "    for L in lags:\n",
    "        if L < 0: cors.append(np.corrcoef(xz[-L:].values, yz[:len(yz)+L].values)[0,1])\n",
    "        elif L>0: cors.append(np.corrcoef(xz[:len(xz)-L].values, yz[L:].values)[0,1])\n",
    "        else:     cors.append(np.corrcoef(xz.values, yz.values)[0,1])\n",
    "    best_idx = int(np.nanargmax(np.abs(cors))); best_lag = int(lags[best_idx]); best_corr = float(cors[best_idx])\n",
    "    sign = \"wind leads Hs\" if best_lag > 0 else (\"Hs leads wind\" if best_lag < 0 else \"simultaneous\")\n",
    "    print(f\"{title}: best lag = {best_lag:+d} h ({sign}),  corr = {best_corr:.3f}\")\n",
    "    plt.figure(figsize=(6,3)); plt.stem(lags, cors)\n",
    "    plt.xlabel('Lag (hours): positive = wind leads Hs'); plt.ylabel('Correlation'); plt.title(title); plt.tight_layout()\n",
    "\n",
    "# Examples (keep your originals)\n",
    "best_cross_correlation(oti.loc[win_mask, 'wind_ms'], oti.loc[win_mask, 'hs'],\n",
    "                       max_lag_hours=48, title='OTI (wind_ms vs Hs, ±3d)')\n",
    "for s, ts in sector_frames.items():\n",
    "    best_cross_correlation(ts.loc[win_mask, 'wind_ms'], ts.loc[win_mask, 'hs'],\n",
    "                           max_lag_hours=48, title=f'{s} (wind_ms vs Hs, ±3d)')\n",
    "\n",
    "save_all_open_figs(prefix=\"storm_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97cef1e-f821-4be9-9df2-d25ddacaff76",
   "metadata": {},
   "source": [
    "## 10) Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "759f9c5e-a576-4f8b-b145-22e52c3aab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- imports needed for this block ---\n",
    "\n",
    "def _save_current_fig(fname_stem: str):\n",
    "    \"\"\"Save the current matplotlib figure to PNG with tight layout.\"\"\"\n",
    "    out = FIGS_DIR / f\"{fname_stem}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
    "    print(\"saved:\", out.resolve())\n",
    "\n",
    "# --- Single-sector exporters --------------------------------------------------\n",
    "def export_sector_timeplot(sector, ts, mask=in_evt):\n",
    "    title = f\"{sector} sector: Hs vs Time (peak window)\"\n",
    "    plot_hs_time_with_reg(ts, title=title, mask=mask)\n",
    "    _save_current_fig(f\"{sector}_time_hs_peak\")\n",
    "\n",
    "def export_sector_wind_vs_hs(sector, ts, mask=in_evt):\n",
    "    d = ts.loc[mask, ['wind_ms','hs']].dropna()\n",
    "    if len(d) < 5:\n",
    "        print(f\"[{sector}] Not enough points for wind-vs-hs.\")\n",
    "        return\n",
    "    x, y = d['wind_ms'].values, d['hs'].values\n",
    "    b1, b0 = np.polyfit(x, y, 1)\n",
    "    xr = np.linspace(x.min(), x.max(), 100)\n",
    "    fig, ax = plt.subplots(figsize=(4.8,4))\n",
    "    ax.scatter(x, y, s=12, alpha=0.8, label='Hourly points')\n",
    "    ax.plot(xr, b1*xr + b0, lw=2, label=f'Fit: Hs = {b1:.3f}·wind + {b0:.3f}')\n",
    "    r = np.corrcoef(x, y)[0,1]\n",
    "    ax.set_xlabel('Wind speed (m/s)'); ax.set_ylabel('Hs (m)')\n",
    "    ax.set_title(f\"{sector} sector: Wind vs Hs (event)\\nPearson r={r:.2f}\")\n",
    "    ax.legend()\n",
    "    _save_current_fig(f\"{sector}_scatter_wind_vs_hs_peak\")\n",
    "\n",
    "def export_sector_rose(sector, ts):\n",
    "    directional_rose_pre_vs_peak(ts, f'{sector} sector')\n",
    "    _save_current_fig(f\"{sector}_rose_pre_vs_peak\")\n",
    "\n",
    "def export_sector_xcorr(sector, ts, mask=win_mask):\n",
    "    crosscorr_plot(ts.loc[mask, 'wind_ms'], ts.loc[mask, 'hs'],\n",
    "                   max_lag_hours=48, title=f'{sector} (wind vs Hs, ±3d)')\n",
    "    _save_current_fig(f\"{sector}_xcorr_wind_hs_pm3d\")\n",
    "\n",
    "# --- Multi-series exporters ---------------------------------------------------\n",
    "def export_map_and_bars():\n",
    "    map_reefs_cyclone_maxhs(sector_centroids, sector_frames, df_cyc)\n",
    "    _save_current_fig(\"map_cyclone_track_and_sectors_maxHs\")\n",
    "\n",
    "    bar_energy_flux_by_sector(sector_frames, mask=in_evt, stat='mean')\n",
    "    _save_current_fig(\"bar_energy_flux_mean_event\")\n",
    "\n",
    "# --- OTI buoy exporters -------------------------------------------------------\n",
    "def export_oti_all():\n",
    "    plot_hs_time_with_reg(oti, title='OTI buoy: Hs vs Time (peak window)', mask=in_evt)\n",
    "    _save_current_fig(\"OTI_time_hs_peak\")\n",
    "\n",
    "    d = oti.loc[in_evt, ['wind_ms','hs']].dropna()\n",
    "    if len(d) >= 5:\n",
    "        x, y = d['wind_ms'].values, d['hs'].values\n",
    "        b1, b0 = np.polyfit(x, y, 1)\n",
    "        xr = np.linspace(x.min(), x.max(), 100)\n",
    "        fig, ax = plt.subplots(figsize=(4.8,4))\n",
    "        ax.scatter(x, y, s=12, alpha=0.8, label='Hourly points')\n",
    "        ax.plot(xr, b1*xr + b0, lw=2, label=f'Fit: Hs = {b1:.3f}·wind + {b0:.3f}')\n",
    "        r = np.corrcoef(x, y)[0,1]\n",
    "        ax.set_xlabel('Wind speed (m/s)'); ax.set_ylabel('Hs (m)')\n",
    "        ax.set_title('OTI buoy: Wind vs Hs (event)\\nPearson r={:.2f}'.format(r))\n",
    "        ax.legend()\n",
    "        _save_current_fig(\"OTI_scatter_wind_vs_hs_peak\")\n",
    "\n",
    "    directional_rose_pre_vs_peak(oti, 'OTI buoy')\n",
    "    _save_current_fig(\"OTI_rose_pre_vs_peak\")\n",
    "\n",
    "    crosscorr_plot(oti.loc[win_mask, 'wind_ms'], oti.loc[win_mask, 'hs'],\n",
    "                   max_lag_hours=48, title='OTI (wind vs Hs, ±3d)')\n",
    "    _save_current_fig(\"OTI_xcorr_wind_hs_pm3d\")\n",
    "\n",
    "# --- Batch: regenerate everything with corrected labels ----------------------\n",
    "def export_all_figs():\n",
    "    export_map_and_bars()\n",
    "    export_oti_all()\n",
    "    for sector, ts in sector_frames.items():\n",
    "        export_sector_timeplot(sector, ts, mask=in_evt)\n",
    "        export_sector_wind_vs_hs(sector, ts, mask=in_evt)\n",
    "        export_sector_rose(sector, ts)\n",
    "        export_sector_xcorr(sector, ts, mask=win_mask)\n",
    "\n",
    "# Run once to regenerate with swapped North/South labels:\n",
    "# export_all_figs()\n",
    "\n",
    "# --- Plotting helpers (unchanged except imports/typo fix) --------------------\n",
    "def plot_hs_time_with_reg(series, title='Hs vs Time (with linear trend)', mask=None):\n",
    "    \"\"\"Line plot of Hs with simple linear fit over selected window.\"\"\"\n",
    "    d = series[['hs']].copy()\n",
    "    if mask is not None:\n",
    "        d = d.loc[mask]\n",
    "    d = d.dropna()\n",
    "    if d.empty:\n",
    "        print(f\"[{title}] No data.\")\n",
    "        return\n",
    "    t0_ = d.index[0]\n",
    "    x = (d.index - t0_).total_seconds() / 3600.0\n",
    "    y = d['hs'].values\n",
    "    X = np.vstack([np.ones_like(x), x]).T\n",
    "    coef = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    yhat = X @ coef\n",
    "    fig, ax = plt.subplots(figsize=(9,3))\n",
    "    ax.plot(d.index, y, lw=1, label='Hs')\n",
    "    ax.plot(d.index, yhat, lw=2, label='Linear trend')\n",
    "    ax.set_title(title); ax.set_ylabel('Hs (m)')\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%m-%d\\n%H:%M'))\n",
    "    ax.legend(); plt.tight_layout()\n",
    "\n",
    "def map_reefs_cyclone_maxhs(sector_centroids, sector_frames, df_cyc, oti_lat=-23.504, oti_lon=152.094):\n",
    "    \"\"\"Simple lon/lat map: cyclone track + sector centroid with max Hs label.\"\"\"\n",
    "    rows = []\n",
    "    for _, r in sector_centroids.iterrows():\n",
    "        name = r['sector']; ts = sector_frames.get(name)\n",
    "        if ts is None or 'hs' not in ts:\n",
    "            continue\n",
    "        hs_max = ts.loc[in_evt, 'hs'].max()\n",
    "        rows.append((name, float(r['sector_lat']), float(r['sector_lon']), hs_max))\n",
    "    df_max = pd.DataFrame(rows, columns=['sector','lat','lon','hs_max'])\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.plot(df_cyc['cyc_lon'], df_cyc['cyc_lat'], lw=1.5, label='Cyclone track')\n",
    "    ax.scatter(df_max['lon'], df_max['lat'], s=80, label='Reef sectors')\n",
    "    for _, r in df_max.iterrows():\n",
    "        ax.text(r['lon']+0.02, r['lat']+0.02, f\"{r['sector']} (max Hs={r['hs_max']:.1f} m)\", fontsize=9)\n",
    "    ax.scatter([oti_lon],[oti_lat], marker='^', s=100, label='OTI buoy')\n",
    "    ax.text(oti_lon+0.02, oti_lat+0.02, \"OTI\", fontsize=9)\n",
    "    all_lons = np.r_[df_max['lon'].values, df_cyc['cyc_lon'].values, oti_lon]\n",
    "    all_lats = np.r_[df_max['lat'].values, df_cyc['cyc_lat'].values, oti_lat]\n",
    "    ax.set_xlim(all_lons.min()-0.3, all_lons.max()+0.3)\n",
    "    ax.set_ylim(all_lats.min()-0.3, all_lats.max()+0.3)\n",
    "    ax.set_xlabel('Longitude'); ax.set_ylabel('Latitude')\n",
    "    ax.set_title('Cyclone track + sectors (max Hs during event)')\n",
    "    ax.legend(); ax.grid(True, ls=':'); plt.tight_layout()\n",
    "\n",
    "def bar_energy_flux_by_sector(sector_frames, mask=in_evt, stat='mean'):\n",
    "    \"\"\"Bar chart of sector energy flux (mean or max) over mask.\"\"\"\n",
    "    vals = []\n",
    "    for s, ts in sector_frames.items():\n",
    "        if 'energy_flux' not in ts:\n",
    "            continue\n",
    "        v = ts.loc[mask, 'energy_flux'].dropna()\n",
    "        if v.empty:\n",
    "            continue\n",
    "        vals.append((s, v.max() if stat == 'max' else v.mean()))\n",
    "    dfp = pd.DataFrame(vals, columns=['sector','P']).sort_values('P', ascending=False)\n",
    "    fig, ax = plt.subplots(figsize=(6,3))\n",
    "    ax.bar(dfp['sector'], dfp['P'])\n",
    "    ax.set_ylabel('Energy flux P (W/m)')\n",
    "    ax.set_title(f'Energy flux per sector ({stat} during event)')\n",
    "    plt.tight_layout()\n",
    "    return dfp\n",
    "\n",
    "def rose_plot(ax, angles_deg, bins=16, title=''):\n",
    "    \"\"\"Polar histogram of directions.\"\"\"\n",
    "    a = np.asarray(angles_deg, dtype=float)\n",
    "    a = a[~np.isnan(a)]\n",
    "    if len(a) == 0:\n",
    "        ax.set_title(title + ' (no data)')\n",
    "        return\n",
    "    theta = np.deg2rad(a % 360.0)\n",
    "    edges = np.linspace(0, 2*np.pi, bins+1)\n",
    "    hist, _ = np.histogram(theta, bins=edges)\n",
    "    widths = np.diff(edges)\n",
    "    ax.bar(edges[:-1], hist, width=widths, align='edge', edgecolor='k', linewidth=0.5)\n",
    "    ax.set_title(title)\n",
    "\n",
    "def directional_rose_pre_vs_peak(df, name='Series'):\n",
    "    \"\"\"Two-panel rose plot: pre vs peak.\"\"\"\n",
    "    pre  = df.loc[pre_evt,  'dp'].dropna() if 'dp' in df else pd.Series(dtype=float)\n",
    "    peak = df.loc[in_evt,   'dp'].dropna() if 'dp' in df else pd.Series(dtype=float)\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    ax1 = plt.subplot(1,2,1, projection='polar'); rose_plot(ax1, pre.values,  bins=16, title=f'{name}: pre')\n",
    "    ax2 = plt.subplot(1,2,2, projection='polar'); rose_plot(ax2, peak.values, bins=16, title=f'{name}: peak')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# --- Example visual calls (unchanged) ----------------------------------------\n",
    "plot_hs_time_with_reg(oti,  title='OTI buoy: Hs vs Time (peak window)', mask=in_evt)\n",
    "for s, ts in sector_frames.items():\n",
    "    plot_hs_time_with_reg(ts, title=f'{s} sector: Hs vs Time (peak window)', mask=in_evt)\n",
    "map_reefs_cyclone_maxhs(sector_centroids, sector_frames, df_cyc)\n",
    "dfp_mean = bar_energy_flux_by_sector(sector_frames, mask=in_evt, stat='mean')\n",
    "directional_rose_pre_vs_peak(oti, 'OTI buoy')\n",
    "for s, ts in sector_frames.items():\n",
    "    directional_rose_pre_vs_peak(ts, f'{s} sector')\n",
    "crosscorr_plot(oti.loc[win_mask, 'wind_ms'], oti.loc[win_mask, 'hs'],\n",
    "               max_lag_hours=48, title='OTI (wind vs Hs, ±3d)')\n",
    "for s, ts in sector_frames.items():\n",
    "    crosscorr_plot(ts.loc[win_mask, 'wind_ms'], ts.loc[win_mask, 'hs'],\n",
    "                   max_lag_hours=48, title=f'{s} (wind vs Hs, ±3d)')\n",
    "save_all_open_figs(prefix=\"visualisations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e4f9c-8fa0-4a94-b26c-73286c7aa9b7",
   "metadata": {},
   "source": [
    "## 11) Model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b5d40e3-9b83-42ab-9106-a01c4ee9c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_diagnostics(df, ycol='hs', xcols=('wind_ms','dist_km','pressure_anom'),\n",
    "                          mask=None, title='Model diagnostics'):\n",
    "    \"\"\"OLS fit + R²/DW/Ljung–Box + residual plots.\"\"\"\n",
    "    d = df.loc[mask, [ycol, *xcols]].dropna()\n",
    "    if len(d) < (len(xcols) + 5):\n",
    "        print(f\"[{title}] Not enough rows (n={len(d)})\")\n",
    "        return None\n",
    "    X = sm.add_constant(d[list(xcols)], has_constant='add')\n",
    "    y = d[ycol]\n",
    "    m = sm.OLS(y, X).fit()\n",
    "    res = m.resid\n",
    "\n",
    "    print(f\"\\n[{title}] R²={m.rsquared:.3f} | adj.R²={m.rsquared_adj:.3f} | n={int(m.nobs)}\")\n",
    "    print(f\"  Durbin–Watson: {sm.stats.durbin_watson(res):.2f}\")\n",
    "    for lag in (6,12,24):\n",
    "        lb = acorr_ljungbox(res, lags=[lag], return_df=True)['lb_pvalue'].iloc[0]\n",
    "        print(f\"  Ljung–Box lag {lag}: p={lb:.3f}\")\n",
    "\n",
    "    fig = plt.figure(figsize=(12,3.2))\n",
    "    plt.subplot(1,3,1); plt.scatter(m.fittedvalues, res, s=10); plt.axhline(0, ls='--', lw=1)\n",
    "    plt.xlabel('Fitted'); plt.ylabel('Residual'); plt.title('Residuals vs Fitted')\n",
    "    plt.subplot(1,3,2); sm.qqplot(res, line='45', fit=True, ax=plt.gca()); plt.title('QQ plot')\n",
    "    plt.subplot(1,3,3); sm.graphics.tsa.plot_acf(res, lags=36, ax=plt.gca()); plt.title('Residual ACF (36 lags)')\n",
    "    fig.suptitle(title, y=1.05, fontsize=11); plt.tight_layout()\n",
    "    return m\n",
    "\n",
    "# Diagnostics for OTI buoy + sectors (peak)\n",
    "m_oti = model_fit_diagnostics(oti, xcols=('wind_ms','dist_km','pressure_anom'),\n",
    "                              mask=in_evt, title='OTI buoy: Hs ~ wind+dist+press (peak)')\n",
    "models_sector = {s: model_fit_diagnostics(ts, xcols=('wind_ms','dist_km','pressure_anom'),\n",
    "                                          mask=in_evt, title=f'{s} sector: Hs ~ wind+dist+press (peak)')\n",
    "                 for s, ts in sector_frames.items()}\n",
    "\n",
    "save_all_open_figs(prefix=\"Diagnostics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e515b1d-8517-40bd-85b9-19d7b421a1fd",
   "metadata": {},
   "source": [
    "## 12) Statistical tests (energy flux & circular ANOVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c7351be-4928-466c-ba7a-de560bed023b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F': 6.365450602179527,\n",
       " 'df1': 2,\n",
       " 'df2': 1208,\n",
       " 'p': 0.001778220325484958,\n",
       " 'n': {'pre': 480, 'peak': 379, 'post': 352},\n",
       " 'means_deg': {'pre': 52.34966826527471,\n",
       "  'peak': 48.44442269056289,\n",
       "  'post': 51.573152081202494},\n",
       " 'kappa_hat': 12.51315110882625,\n",
       " 'Rbar': 0.959174663300831}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Imports (deduped) -------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "\n",
    "# --- Energy flux: summary + pairwise tests -----------------------------------\n",
    "def energy_flux_tests_all(sector_frames, mask, sectors=('North', 'Central', 'South')):\n",
    "    \"\"\"\n",
    "    Summarize energy_flux per sector over `mask`, then run pairwise tests:\n",
    "      - Welch t-test (unequal variances)\n",
    "      - Mann–Whitney U (two-sided)\n",
    "    \"\"\"\n",
    "    # Pull series and basic diagnostics\n",
    "    P = {}\n",
    "    for s in sectors:\n",
    "        if s not in sector_frames:\n",
    "            print(f\"[warn] sector '{s}' not found in sector_frames.\")\n",
    "            continue\n",
    "        ts = sector_frames[s]\n",
    "        if 'energy_flux' not in ts.columns:\n",
    "            print(f\"[warn] sector '{s}' has no 'energy_flux' column.\")\n",
    "            P[s] = pd.Series(dtype=float)\n",
    "            continue\n",
    "        P[s] = ts.loc[mask, 'energy_flux'].dropna()\n",
    "\n",
    "    # Summary table\n",
    "    print(\"\\nEnergy flux samples per sector:\")\n",
    "    for s in sectors:\n",
    "        v = P.get(s, pd.Series(dtype=float))\n",
    "        print(f\"  {s:7s}  n={len(v):4d}  mean={v.mean():.1f}  sd={v.std(ddof=1):.1f}\")\n",
    "\n",
    "    # Guard: need at least 2 sectors with ≥3 samples\n",
    "    valid = [s for s in sectors if len(P.get(s, [])) >= 3]\n",
    "    if len(valid) < 2:\n",
    "        print(\"\\nNot enough data for pairwise tests (need ≥3 pts in at least two sectors).\")\n",
    "        return\n",
    "\n",
    "    # Pairwise tests\n",
    "    print(\"\\nPairwise tests (Welch t-test | Mann–Whitney U):\")\n",
    "    for a, b in combinations(valid, 2):\n",
    "        va, vb = P[a], P[b]\n",
    "        t_w, p_w = stats.ttest_ind(va, vb, equal_var=False, nan_policy='omit')\n",
    "        u, p_u = stats.mannwhitneyu(va, vb, alternative='two-sided')\n",
    "        print(f\"  {a:7s} vs {b:7s}  t={t_w:7.3f}, p={p_w:.4g}   |   U={u:7.0f}, p={p_u:.4g}\")\n",
    "\n",
    "def energy_flux_samples_oti(mask=in_evt, label=\"OTI\"):\n",
    "    \"\"\"\n",
    "    Print OTI energy-flux sample size, mean, and SD over `mask`,\n",
    "    matching the sector summary format.\n",
    "    \"\"\"\n",
    "    # pick the right column (you computed P_buoy earlier)\n",
    "    frame = oti if 'oti' in globals() else oti_frame\n",
    "    col = 'energy_flux' if 'energy_flux' in frame.columns else ('P_buoy' if 'P_buoy' in frame.columns else None)\n",
    "    if col is None:\n",
    "        print(f\"{label}: no 'energy_flux' or 'P_buoy' column found.\")\n",
    "        return\n",
    "\n",
    "    v = frame.loc[mask, col].dropna()\n",
    "    print(\"\\nEnergy flux samples (OTI):\")\n",
    "    print(f\"  {label:7s} n={len(v):4d}  mean={v.mean():.1f}  sd={v.std(ddof=1):.1f}\")\n",
    "\n",
    "\n",
    "# --- Watson–Williams (circular ANOVA) implementation (degrees) ---------------\n",
    "def _estimate_kappa(Rbar):\n",
    "    \"\"\"Approximate kappa from mean resultant length Rbar (Zar/Berens).\"\"\"\n",
    "    if Rbar < 0.53:\n",
    "        return 2*Rbar + Rbar**3 + (5*Rbar**5)/6\n",
    "    elif Rbar < 0.85:\n",
    "        return -0.4 + 1.39*Rbar + 0.43/(1 - Rbar)\n",
    "    else:\n",
    "        # avoid div by zero\n",
    "        eps = 1e-12\n",
    "        return 1.0 / max(Rbar**3 - 4*Rbar**2 + 3*Rbar, eps)\n",
    "\n",
    "def watson_williams_test(data_dict):\n",
    "    \"\"\"\n",
    "    Watson–Williams test for equality of circular means across groups.\n",
    "    data_dict: {'group': array_like of angles in degrees, ...}\n",
    "    Returns dict: F, df1, df2, p, per-group n and mean directions (deg).\n",
    "    \"\"\"\n",
    "    # Clean groups: wrap to [0, 360), drop NaNs, require n>=2\n",
    "    groups = {}\n",
    "    for g, a in data_dict.items():\n",
    "        if a is None:\n",
    "            continue\n",
    "        x = np.asarray(a, dtype=float)\n",
    "        x = x[np.isfinite(x)]\n",
    "        if x.size >= 2:\n",
    "            groups[g] = np.mod(x, 360.0)\n",
    "\n",
    "    k = len(groups)\n",
    "    if k < 2:\n",
    "        raise ValueError(\"Need at least two groups with >=2 observations each.\")\n",
    "\n",
    "    # Convert to radians\n",
    "    rad = {g: np.deg2rad(v) for g, v in groups.items()}\n",
    "\n",
    "    # Per-group sums\n",
    "    n_i = {g: v.size for g, v in rad.items()}\n",
    "    C_i = {g: np.sum(np.cos(v)) for g, v in rad.items()}\n",
    "    S_i = {g: np.sum(np.sin(v)) for g, v in rad.items()}\n",
    "    R_i = {g: np.hypot(C_i[g], S_i[g]) for g in groups}\n",
    "\n",
    "    # Pooled\n",
    "    N = np.sum(list(n_i.values()))\n",
    "    C = np.sum(list(C_i.values()))\n",
    "    S = np.sum(list(S_i.values()))\n",
    "    R = np.hypot(C, S)\n",
    "\n",
    "    # Concentration estimate and small-sample correction\n",
    "    Rbar = R / N\n",
    "    kappa = _estimate_kappa(Rbar)\n",
    "    c = 1.0 + 3.0/(8.0 * max(kappa, 1e-12))  # small-sample correction\n",
    "\n",
    "    # WW F-statistic (Berens' CircStat style)\n",
    "    df1 = k - 1\n",
    "    df2 = N - k\n",
    "    num = (N - k) * (np.sum(list(R_i.values())) - R)\n",
    "    den = (k - 1) * (N - np.sum(list(R_i.values())))\n",
    "    if den <= 0:\n",
    "        F = 0.0\n",
    "        p = 1.0\n",
    "    else:\n",
    "        F = c * (num / den)\n",
    "        F = float(max(F, 0.0))  # clamp tiny negatives\n",
    "        p = stats.f.sf(F, df1, df2)\n",
    "\n",
    "    # Group circular means (deg)\n",
    "    means_deg = {}\n",
    "    for g in groups:\n",
    "        mu = np.degrees(np.arctan2(S_i[g]/n_i[g], C_i[g]/n_i[g])) % 360.0\n",
    "        means_deg[g] = mu\n",
    "\n",
    "    return {\n",
    "        'F': F, 'df1': df1, 'df2': df2, 'p': p,\n",
    "        'n': n_i, 'means_deg': means_deg, 'kappa_hat': kappa, 'Rbar': Rbar\n",
    "    }\n",
    "\n",
    "# --- Wrapper: use Pingouin if available, else built-in WW --------------------\n",
    "def circ_anova_by_group(data_dict):\n",
    "    \"\"\"\n",
    "    Try Pingouin's `watson_williams` if present; else use the built-in\n",
    "    Watson–Williams implementation above. Prints a compact summary.\n",
    "    \"\"\"\n",
    "    # Tidy: drop empty groups early\n",
    "    cleaned = {g: pd.Series(a, dtype=float).dropna().values for g, a in data_dict.items()}\n",
    "    cleaned = {g: a for g, a in cleaned.items() if a.size >= 2}\n",
    "    if len(cleaned) < 2:\n",
    "        print(\"No directional data available for circular ANOVA (need ≥2 groups with ≥2 obs).\")\n",
    "        return None\n",
    "\n",
    "    # Prefer Pingouin if available in this environment\n",
    "    try:\n",
    "        import pingouin as pg\n",
    "        if hasattr(pg, 'watson_williams'):\n",
    "            df = pd.DataFrame(\n",
    "                [{'dp': float(v), 'group': g}\n",
    "                 for g, arr in cleaned.items()\n",
    "                 for v in np.mod(arr, 360.0)]\n",
    "            )\n",
    "            res = pg.watson_williams(dv='dp', between='group', data=df)\n",
    "            print(\"\\nCircular ANOVA (Watson–Williams) via Pingouin:\")\n",
    "            print(res)\n",
    "            return res\n",
    "    except Exception:\n",
    "        # fall through to built-in\n",
    "        pass\n",
    "\n",
    "    # Built-in implementation\n",
    "    ww = watson_williams_test(cleaned)\n",
    "    print(\"\\nCircular ANOVA (Watson–Williams) – built-in implementation\")\n",
    "    print(f\"F({ww['df1']}, {ww['df2']}) = {ww['F']:.3f},  p = {ww['p']:.4g}\")\n",
    "    print(\"Group means (deg):\", {k: round(v, 1) for k, v in ww['means_deg'].items()})\n",
    "    return ww\n",
    "\n",
    "# --- Convenience: run by sector (peak) + by phase for any series -------------\n",
    "# Example call for sectors (peak window):\n",
    "groups_peak = {s: ts.loc[in_evt, 'dp'].values for s, ts in sector_frames.items() if 'dp' in ts}\n",
    "circ_anova_by_group(groups_peak)\n",
    "\n",
    "def anova_by_phase(ts, name='Series'):\n",
    "    \"\"\"Watson–Williams on dp across phases (pre/peak/post) for a single series.\"\"\"\n",
    "    ph = {}\n",
    "    if 'dp' in ts:\n",
    "        ph['pre']  = ts.loc[pre_evt,  'dp'].values\n",
    "        ph['peak'] = ts.loc[in_evt,   'dp'].values\n",
    "        ph['post'] = ts.loc[post_evt, 'dp'].values\n",
    "    print(f\"\\n{name} – Circular ANOVA of dp by phase (pre/peak/post):\")\n",
    "    return circ_anova_by_group(ph)\n",
    "\n",
    "# Example calls:\n",
    "anova_by_phase(sector_frames['North'], 'North')\n",
    "anova_by_phase(sector_frames['Central'], 'Central')\n",
    "anova_by_phase(sector_frames['South'], 'South')\n",
    "anova_by_phase(oti, 'OTI buoy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730dad80-fa0f-4df4-a8f0-557a1c9d2ec5",
   "metadata": {},
   "source": [
    "## 13) Quick sanity prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff98ff50-0b9d-4e5a-a418-160f298e3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nWindows:\")\n",
    "print(\"  OTI (buoy) idx:\", oti.index.min(), \"→\", oti.index.max(), \"| n:\", len(oti))\n",
    "print(\"  Cyclone peak: \", t_first, \"→\", t_last)\n",
    "print(\"  Regr rows (peak):\", len(df_reg))\n",
    "print(\"dims:\", ds_wave.dims)\n",
    "print(\"coords:\", list(ds_wave.coords))\n",
    "print(\"data vars (first 10):\", list(ds_wave.data_vars)[:10])\n",
    "try:\n",
    "    print(\"lat shape:\", ds_wave['lat'].shape)\n",
    "    print(\"lon shape:\", ds_wave['lon'].shape)\n",
    "except Exception as e:\n",
    "    print(\"No 'lat'/'lon' variable:\", e)\n",
    "print(\"Raw buoy headers:\", list(df_buoy.columns)[:20])\n",
    "print(df_buoy.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0a313-1645-45df-a0df-15059865f0c2",
   "metadata": {},
   "source": [
    "## 14) Save Printed Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35e4fe83-7119-4c4d-8504-51c62f3e720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ---- Log destination\n",
    "LOG_DIR = Path(\"logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Helpers to access OTI and make sure energy flux exists\n",
    "def _get_oti_like():\n",
    "    \"\"\"Return (DataFrame, label) for the OTI buoy table present in globals().\"\"\"\n",
    "    if 'oti' in globals():\n",
    "        return oti, \"OTI buoy\"\n",
    "    if 'oti_frame' in globals():\n",
    "        return oti_frame, \"OTI buoy\"\n",
    "    raise NameError(\"Neither `oti` nor `oti_frame` is defined.\")\n",
    "\n",
    "def energy_flux_samples_oti(mask, label=\"OTI\"):\n",
    "    \"\"\"Print OTI energy-flux sample size, mean, and SD over mask (matches sector format).\"\"\"\n",
    "    frame, _ = _get_oti_like()\n",
    "    col = 'energy_flux' if 'energy_flux' in frame.columns else ('P_buoy' if 'P_buoy' in frame.columns else None)\n",
    "    if col is None:\n",
    "        print(f\"{label}: no 'energy_flux' or 'P_buoy' column found.\")\n",
    "        return\n",
    "    v = frame.loc[mask, col].dropna()\n",
    "    print(\"\\nEnergy flux samples (OTI):\")\n",
    "    print(f\"  {label:7s} n={len(v):4d}  mean={v.mean():.1f}  sd={v.std(ddof=1):.1f}\")\n",
    "\n",
    "# Ensure OTI has an 'energy_flux' column if only P_buoy is present\n",
    "try:\n",
    "    _oti_df, _ = _get_oti_like()\n",
    "    if 'energy_flux' not in _oti_df.columns and 'P_buoy' in _oti_df.columns:\n",
    "        _oti_df = _oti_df.copy()\n",
    "        _oti_df['energy_flux'] = _oti_df['P_buoy']\n",
    "        if 'oti' in globals():\n",
    "            oti = _oti_df\n",
    "        else:\n",
    "            oti_frame = _oti_df\n",
    "except Exception:\n",
    "    pass  # if OTI not present yet, skip\n",
    "\n",
    "# ---- Directional shift summaries\n",
    "def _print_dirshift_sectors():\n",
    "    print(\"\\n=== Directional Shift Analysis (per sector) ===\")\n",
    "    rows_dir = []\n",
    "    for sector, ts in sector_frames.items():\n",
    "        pre  = ts.loc[pre_evt,  'dp'].dropna() if 'dp' in ts else pd.Series(dtype=float)\n",
    "        peak = ts.loc[in_evt,   'dp'].dropna() if 'dp' in ts else pd.Series(dtype=float)\n",
    "        post = ts.loc[post_evt, 'dp'].dropna() if 'dp' in ts else pd.Series(dtype=float)\n",
    "        pre_m, peak_m, post_m = circ_mean_deg(pre), circ_mean_deg(peak), circ_mean_deg(post)\n",
    "        d_pre_peak  = circ_diff(pre_m,  peak_m)\n",
    "        d_peak_post = circ_diff(peak_m, post_m)\n",
    "        d_pre_post  = circ_diff(pre_m,  post_m)\n",
    "        print(f\"{sector:8s}  mean°  pre:{pre_m:6.1f}  peak:{peak_m:6.1f}  post:{post_m:6.1f}  \"\n",
    "              f\"Δ(pre→peak):{d_pre_peak:6.1f}  Δ(peak→post):{d_peak_post:6.1f}  Δ(pre→post):{d_pre_post:6.1f}\")\n",
    "        rows_dir.append({'sector': sector, 'pre_mean': pre_m, 'peak_mean': peak_m, 'post_mean': post_m,\n",
    "                         'Δ_pre_peak': d_pre_peak, 'Δ_peak_post': d_peak_post, 'Δ_pre_post': d_pre_post})\n",
    "    df_dirshift = pd.DataFrame(rows_dir)\n",
    "    if not df_dirshift.empty:\n",
    "        print(\"\\nSector directional shift summary (rounded):\")\n",
    "        with pd.option_context('display.max_columns', None):\n",
    "            print(df_dirshift.round(1))\n",
    "\n",
    "def _print_dirshift_oti():\n",
    "    print(\"\\n=== Directional Shift Analysis (OTI buoy) ===\")\n",
    "    oti_like, label = _get_oti_like()\n",
    "    if 'dp' not in oti_like.columns:\n",
    "        print(f\"[{label}] No 'dp' column found — skipping.\")\n",
    "        return\n",
    "    pre_dp  = oti_like.loc[pre_mask,  'dp'].dropna()\n",
    "    peak_dp = oti_like.loc[peak_mask, 'dp'].dropna()\n",
    "    post_dp = oti_like.loc[post_mask, 'dp'].dropna()\n",
    "    pre_m  = circ_mean_deg(pre_dp.values)\n",
    "    peak_m = circ_mean_deg(peak_dp.values)\n",
    "    post_m = circ_mean_deg(post_dp.values)\n",
    "    print(f\"{label:8s} mean°  pre:{pre_m:6.1f}  peak:{peak_m:6.1f}  post:{post_m:6.1f}  \"\n",
    "          f\"Δ(pre→peak):{circ_diff(pre_m,peak_m):6.1f}  Δ(peak→post):{circ_diff(peak_m,post_m):6.1f}  \"\n",
    "          f\"Δ(pre→post):{circ_diff(pre_m,post_m):6.1f}\")\n",
    "    print(f\"           n      pre:{len(pre_dp):6d}  peak:{len(peak_dp):6d}  post:{len(post_dp):6d}\")\n",
    "\n",
    "# ---- Regressions (sector + OTI)\n",
    "def single_var_reg(ts, xvar, mask=None):\n",
    "    \"\"\"Quick single-var regression: Hs ~ xvar within mask (defaults to in_evt).\"\"\"\n",
    "    if mask is None:\n",
    "        mask = in_evt\n",
    "    if xvar not in ts.columns or 'hs' not in ts.columns:\n",
    "        return None\n",
    "    d = ts.loc[mask, ['hs', xvar]].copy()\n",
    "    d[xvar] = pd.to_numeric(d[xvar], errors='coerce')\n",
    "    d['hs'] = pd.to_numeric(d['hs'], errors='coerce')\n",
    "    d = d.dropna()\n",
    "    if len(d) < 10:\n",
    "        return None\n",
    "    X = sm.add_constant(d[xvar], has_constant='add')\n",
    "    m = sm.OLS(d['hs'], X).fit()\n",
    "    return xvar, m.rsquared, int(m.nobs), float(m.params.get(xvar, np.nan)), float(m.pvalues.get(xvar, np.nan))\n",
    "\n",
    "def _print_sector_regressions():\n",
    "    print(\"\\n=== Sector regressions ===\")\n",
    "    for sector, ts in sector_frames.items():\n",
    "        try:\n",
    "            m1 = regress_hs_distance(ts, mask=in_evt)          # uses your defined function\n",
    "            m2 = regress_exposure(ts,     mask=in_evt)          # uses your defined function\n",
    "            print(f\"[{sector}] Hs~dist: R2={m1.rsquared:.3f}, n={int(m1.nobs)}\")\n",
    "            print(f\"[{sector}] Hs~wind+dist+press: R2={m2.rsquared:.3f}, n={int(m2.nobs)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{sector}] regression error:\", repr(e))\n",
    "        results = []\n",
    "        for var in ['wind_ms', 'pressure_anom', 'dp']:\n",
    "            out = single_var_reg(ts, var, mask=in_evt)\n",
    "            if out: results.append(out)\n",
    "        if results:\n",
    "            print(f\"{sector} — Hs vs single forcings (cyclone window)\")\n",
    "            for var, r2, n, coef, p in results:\n",
    "                print(f\"  {var:>14} | R²={r2:5.3f} | n={n:3d} | coef={coef:8.3f} | p={p:8.3e}\")\n",
    "\n",
    "def _print_oti_regressions():\n",
    "    print(\"\\n=== OTI regressions ===\")\n",
    "    oti_like, label = _get_oti_like()\n",
    "    _ = oti_regression_suite(oti_like, label=label, mask=peak_mask)  # your existing suite\n",
    "\n",
    "# =========================\n",
    "# Main report writer\n",
    "# =========================\n",
    "def run_text_reports_to_file(filepath: Path):\n",
    "    \"\"\"\n",
    "    Run all text-only reports and write every print to `filepath`.\n",
    "    Captures both stdout and stderr so warnings from statsmodels are included.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f, \\\n",
    "         redirect_stdout(f), \\\n",
    "         redirect_stderr(f), \\\n",
    "         pd.option_context('display.width', 140, 'display.max_columns', None, 'display.max_rows', 500):\n",
    "\n",
    "        print(\"=== Cyclone/Wave Analysis Report ===\")\n",
    "        print(\"Log file:\", filepath)\n",
    "        print(\"Timestamp:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        # Context line: storm + window (t_first/t_last from your cyclone window)\n",
    "        try:\n",
    "            print(f\"Storm: {STORM} | Window: {t_first:%Y-%m-%d %H:%M} → {t_last:%Y-%m-%d %H:%M}\")\n",
    "        except Exception:\n",
    "            print(\"(Storm metadata unavailable)\")\n",
    "\n",
    "        # Energy flux (sectors + OTI one-liner in the same format)\n",
    "        print(\"\\n-- Energy flux tests (pairwise) --\")\n",
    "        energy_flux_tests_all(sector_frames, mask=in_evt)\n",
    "        energy_flux_samples_oti(mask=in_evt, label=\"OTI\")\n",
    "\n",
    "        # Circular ANOVA by sector (peak window)\n",
    "        print(\"\\n-- Circular ANOVA by sector (peak) --\")\n",
    "        groups_peak = {s: ts.loc[in_evt, 'dp'].values for s, ts in sector_frames.items() if 'dp' in ts}\n",
    "        circ_anova_by_group(groups_peak)\n",
    "\n",
    "        # Circular ANOVA by phase (North/Central/South + OTI) — single pass, no duplicates\n",
    "        print(\"\\n-- Circular ANOVA by phase --\")\n",
    "        for sname in ['North', 'Central', 'South']:\n",
    "            anova_by_phase(sector_frames[sname], sname)\n",
    "        oti_like, label = _get_oti_like()\n",
    "        anova_by_phase(oti_like, label)\n",
    "\n",
    "        # Directional shift summaries\n",
    "        print(\"\\n-- Directional shift summaries --\")\n",
    "        _print_dirshift_sectors()\n",
    "        _print_dirshift_oti()\n",
    "\n",
    "        # Regressions (sectors + OTI)\n",
    "        print(\"\\n-- Regressions --\")\n",
    "        _print_sector_regressions()\n",
    "        _print_oti_regressions()\n",
    "\n",
    "        print(\"\\n=== END OF REPORT ===\")\n",
    "\n",
    "# =========================\n",
    "# Execute once to generate a log\n",
    "# =========================\n",
    "log_name = f\"analysis_log_{datetime.now():%Y%m%d_%H%M%S}.txt\"\n",
    "log_path = LOG_DIR / log_name\n",
    "run_text_reports_to_file(log_path)\n",
    "print(\"Saved log to:\", log_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7a320-5342-4cbb-bfd0-fed86d1bb901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
